forum,title,keywords,abstract,pdf,match,authors,venue,year,type
https://openreview.net/forum?id=ryxOh7n9Ir,Low Shot Learning with Untrained Neural Networks for Imaging Inverse Problems,"['Inverse problems', 'compressed sensing', 'low shot learning', 'image priors', 'untrained neural networks', 'generative priors']","Employing deep neural networks as natural image priors to solve inverse problems either requires large amounts of data to sufficiently train expressive generative models or can succeed with no data via untrained neural networks. However, very few works have considered how to interpolate between these no- to high-data regimes. In particular, how can one use the availability of a small amount of data (even 5-25 examples) to one's advantage in solving these inverse problems and can a system's performance increase as the amount of data increases as well? In this work, we consider solving linear inverse problems when given a small number of examples of images that are drawn from the same distribution as the image of interest. Comparing to untrained neural networks that use no data, we show how one can pre-train a neural network with a few given examples to improve reconstruction results in compressed sensing and semantic image recovery problems such as colorization. Our approach leads to improved reconstruction as the amount of available data increases and is on par with fully trained generative models, while requiring less than 1% of the data needed to train a generative model.",https://openreview.net/pdf/a57eb3f8d8f44793cd0ac6e8d4b3e52783fdd232.pdf,stupidity,"['Oscar Leong', 'Wesam Sakla']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryl6nX398r,Gradient-Based Neural DAG Learning,"['causality', 'directed acyclic graph', 'neural network', 'constrained optimization']","We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while being more global in its search compared to other greedy approaches. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks while being competitive with existing greedy search methods on important metrics for causal inference.",https://openreview.net/pdf/fa3d9988ea0255a69eef4fb885e76cbd693271e6.pdf,stupidity,"['Sébastien Lachapelle', 'Philippe Brouillard', 'Tristan Deleu', 'Simon Lacoste-Julien']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryeo27n9Ur,Memory-efficient Learning for Large-scale Computational Imaging,"['Computational Imaging', 'Backpropagation', 'Learning', 'Reverse-mode differentiation', 'microscopy', 'magnetic resonance imaging']","Computational imaging systems jointly design computation and hardware to retrieve information which is not traditionally accessible with standard imaging systems. Recently, critical aspects such as experimental design and image priors are optimized through deep neural networks formed by the unrolled iterations of classical physics-based reconstructions (termed physics-based networks). However, for real-world large-scale systems, computing gradients via backpropagation restricts learning due to memory limitations of graphical processing units. In this work, we propose a memory-efficient learning procedure that exploits the reversibility of the network’s layers to enable data-driven design for large-scale computational imaging. We demonstrate our methods practicality on two large-scale systems: super-resolution optical microscopy and multi-channel magnetic resonance imaging.",https://openreview.net/pdf/42385aefdbfd5ce0d22e7fb792618fe6a02e0b87.pdf,stupidity,"['Michael Kellman', 'Jon Tamir', 'Emrah Bostan', 'Michael Lustig', 'Laura Waller']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkxbn735IH,Robust One-Bit Recovery via ReLU Generative Networks: Improved Statistical Rate and Global Landscape Analysis,"['one-bit compressed sensing', 'ReLU generative network', 'uniform recovery', 'landscape analysis']","We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector
$\theta_0\in\mathbb{R}^d$ \emph{uniformly} from $m$ quantized noisy measurements. Under the assumption that the measurements are sub-Gaussian,  to recover any $k$-sparse $\theta_0$ ($k\ll d$) \emph{uniformly} up to an error $\varepsilon$ with high probability, the best known computationally tractable algorithm requires\footnote{Here, an algorithm is ``computationally tractable'' if it has provable convergence guarantees. The notation $\tilde{\mathcal{O}}(\cdot)$ omits a logarithm factor of $\varepsilon^{-1}$.} $m\geq\tilde{\mathcal{O}}(k\log d/\varepsilon^4)$. In this paper, we consider a new framework for the one-bit sensing problem where the sparsity is implicitly enforced via mapping a low dimensional representation $x_0$ through a known $n$-layer ReLU generative network $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$. Such a framework poses low-dimensional priors on $\theta_0$ without a known basis. We propose to recover the target $G(x_0)$ via an unconstrained empirical risk minimization (ERM) problem under a much weaker \emph{sub-exponential  measurement assumption}.  For such a problem, we establish a joint statistical and computational analysis. In particular, we prove that the ERM estimator in this new framework achieves an improved statistical rate of $m=\tilde{\mathcal{O}} (kn\log d /\epsilon^2)$ recovering any $G(x_0)$ uniformly up to an error $\varepsilon$. Moreover, from the lens of computation, we prove that under proper conditions on the ReLU weights, our proposed empirical risk, despite non-convexity, has no stationary point outside of small neighborhoods around the true representation $x_0$ and its negative multiple. Furthermore, we show that the global minimizer of the empirical risk stays within the neighborhood around $x_0$ rather than its negative multiple. Our analysis sheds some light on the possibility of inverting a deep generative model under partial and quantized measurements, complementing the recent success of using deep generative models for inverse problems.",https://openreview.net/pdf/9dcc4f1c43f70fca12f4514e83aea7c45073c5bf.pdf,stupidity,"['Shuang Qiu', 'Xiaohan Wei', 'Zhuoran Yang']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkeahX3qLr,Co-Generation with GANs using AIS based HMC,"['annealed importance sampling', 'co-generation', 'GAN']","Inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones – which we refer to as co-generation – is an important challenge that is computationally demanding for all but the simplest settings. This task has received a considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction. In contrast, almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs). Therefore, in this paper, we study the occurring challenges for co-generation with GANs. To address those challenges we develop an annealed importance sampling (AIS) based Hamiltonian Monte Carlo (HMC) co-generation algorithm. The presented approach significantly outperforms classical gradient-based methods on synthetic data and on CelebA.",https://openreview.net/pdf/617208a2e157dfcb96ce9d7b423d76f245ec04f6.pdf,stupidity,"['Tiantian Fang', 'Alexander G. Schwing']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJeNnm25US,A GAN based solver of black-box inverse problems,['GAN'],"We propose a GAN based approach to solve inverse problems which have non-differential or non-continuous forward relations. In the standard sense, an inverse problem is interpreted as the process of calculating factors that produce observations. We reformulate the inverse problem such that the discriminator is a binary classifier and the generator is used to produce samples in a local region of the input domain of the forward relation. Our GAN based approach solves inverse problems by using adversarial training but without relying on the gradients of the original problem formulation. We prove the efficacy of our approach by applying it to an artificially generated topology optimization problem. We demonstrate that despite not having access to derivatives of f our method leads to similar results than more traditional topology optimization methods.",https://openreview.net/pdf/a5e6df2ec9d9bb880d7236782b863db9e5bafa83.pdf,stupidity,"['Michael Gillhofer', 'Hubert Ramsauer', 'Johannes Brandstetter', 'Bernhard Schäfl', 'Sepp Hochreiter']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1l9n725IH,Phase Retrieval using Untrained Neural Network Priors,[],"Untrained deep neural networks as image priors have been recently introduced for linear inverse imaging problems such as denoising, super-resolution,  inpainting and compressive sensing with promising performance gains over hand-crafted image priors such as sparsity. Moreover, unlike learned generative priors they do not require any training over large datasets.  In this paper, we consider the problem of solving the non-linear inverse problem of compressive phase retrieval; this involves reconstructing a $d$-dimensional image signal from $n$ magnitude-only measurements, and $n<d$. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present two approaches for solving this problem: vanilla gradient descent and a projected gradient descent scheme and show superior empirical performance when compared to algorithms that use hand crafted priors.",https://openreview.net/pdf/80e4e1dc3e1999e801afa8e4d61cb3447b49ac69.pdf,stupidity,"['Gauri Jagatap', 'Chinmay Hegde']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1gOn7h9UH,Interpretable and robust blind image denoising with bias-free convolutional neural networks,"['Blind image denoising', 'interpretability of deep neural networks', 'Generalization in deep neural networks']","Deep convolutional networks often append additive constant (""bias"") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of ""batch normalization""). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",https://openreview.net/pdf/0807e692618b7bdbb105acd9729c6be9b2863e2b.pdf,stupidity,"['Zahra Kadkhodaie', 'Sreyas Mohan', 'Eero P. Simoncelli', 'Carlos Fernandez-Granda']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyxYnQ398H,Learning to Solve Linear Inverse Problems in Imaging with Neumann Networks,"['Inverse Problems', 'Image Reconstruction', 'Learned Regularizers', 'Neumann series', 'Patch-based methods']","Recent advances have illustrated that it is often possible to learn to solve linear inverse problems in imaging using training data that can outperform more traditional regularized least squares solutions. Along these lines, we present some extensions of the Neumann network, a recently introduced end-to-end learned architecture inspired by a truncated Neumann series expansion of the solution map to a regularized least squares problem. Here we summarize the Neumann network approach, and show that it has a form compatible with the optimal reconstruction function for a given inverse problem. We also investigate an extension of the Neumann network that incorporates a more sample efficient patch-based regularization approach.",https://openreview.net/pdf/16ccf92382cfbd1ab21f44ca483690fb22e20054.pdf,stupidity,"['Greg Ongie', 'Davis Gilton', 'Rebecca Willett']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyxJpm3cLH,Subsampled Fourier Ptychography via Pretrained Invertible and Untrained Network Priors,"['Fourier ptychography', 'Invertible generative models', 'Untrained network priors']","Recently pretrained generative models have shown promising results for subsampled Fourier Ptychography (FP) in terms of quality of reconstruction for extremely low sampling rate and high noise. However, one of the significant drawbacks of these pretrained generative priors is their limited representation capabilities. Moreover, training these generative models requires access to a large number of fully-observed clean samples of a particular class of images like faces or digits that is prohibitive to obtain in the context of FP. In this paper, we propose to leverage the power of pretrained invertible and untrained generative models to mitigate the representation error issue and requirement of a large number of example images (for training generative models) respectively. Through extensive experiments, we demonstrate the effectiveness of proposed approaches in the context of FP for low sampling rates and high noise levels.",https://openreview.net/pdf/128dad0b224e97e389f89c41ae49e2476b45ca4d.pdf,stupidity,"['Fahad Shamshad', 'Asif Hanif', 'Ali Ahmed']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Sygfa739LS,Learning to Recover Sparse Signals,"['Compressed Sensing', 'Reinforcement Learning', 'Monte Carlo Tree Search', 'Basis Pursuit', 'Orthogonal Matching Pursuit']","In compressed sensing, a primary problem to solve is to reconstruct a high dimensional sparse signal from a small number of observations. In this work, we develop a new sparse signal recovery algorithm using reinforcement learning (RL) and Monte CarloTree Search (MCTS). Similarly to orthogonal matching pursuit (OMP), our RL+MCTS algorithm chooses the support of the signal sequentially. The key novelty is that the proposed algorithm learns how to choose the next support as opposed to following a pre-designed rule as in OMP. Empirical results are provided to demonstrate the superior performance of the proposed RL+MCTS algorithm over existing sparse signal recovery algorithms. ",https://openreview.net/pdf/f370f4d4e9f04cdb2fe19c569bfe041a51f9a60f.pdf,stupidity,"['Sichen Zhong', 'Yue Zhao', 'Jianshu Chen']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Sye6om358H,Precise asymptotics for phase retrieval and compressed sensing with random generative priors,"['Phase retrieval', 'compressed sensing', 'generative priors']","We consider the problem of compressed sensing and of (real-valued) phase retrieval with random measurement matrix.
We analyse sharp asymptotics of the information-theoretically optimal performance and that of the best known polynomial algorithms under a generative prior consisting of a single layer neural network with a random weight matrix. 
We compare the performance to sparse separable priors and conclude that generative priors might be advantageous in terms of algorithmic performance. 
In particular, while sparsity does not allow to perform compressive phase retrieval efficiently close to its information-theoretic limit, it is found that under the random generative prior compressed phase retrieval becomes tractable. ",https://openreview.net/pdf/a7752d799b26e6e4995006768e9955e3cefcf400.pdf,stupidity,"['Benjamin Aubin', 'Bruno Loureiro', 'Antoine Baker', 'Florent Krzakala', 'Lenka Zdeborova']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJxRjQncLH,Energy Dissipation with Plug-and-Play Priors,"['deep learning', 'image reconstruction', 'inverse problems', 'denoising', 'variational methods', 'energy minimization']","Neural networks have reached outstanding performance for solving various ill-posed inverse problems in imaging. However, drawbacks of end-to-end learning approaches in comparison to classical variational methods are the requirement of expensive retraining for even slightly different problem statements and the lack of provable error bounds during inference. Recent works tackled the first problem by using networks trained for Gaussian image denoising as generic plug-and-play regularizers in energy minimization algorithms. Even though this obtains state-of-the-art results on many tasks, heavy restrictions on the network architecture have to be made if provable convergence of the underlying fixed point iteration is a requirement. More recent work has proposed to train networks to output descent directions with respect to a given energy function with a provable guarantee of convergence to a minimizer of that energy. However, each problem and energy requires the training of a separate network.
In this paper we consider the combination of both approaches by projecting the outputs of a plug-and-play denoising network onto the cone of descent directions to a given energy. This way, a single pre-trained network can be used for a wide variety of reconstruction tasks. Our results show improvements compared to classical energy minimization methods while still having provable convergence guarantees.",https://openreview.net/pdf/25fd5cf49be93843cca6c75c16412a8359d7b8f4.pdf,stupidity,"['Hendrik Sommerhoff', 'Andreas Kolb', 'Michael Moeller']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJgYiQnq8H,Sample Complexity Lower Bounds for Compressive Sensing with Generative Models,"['Compressive sensing with generative models', 'sample complexity', 'lower bound', 'ReLU networks']","The goal of standard compressive sensing is to estimate an unknown vector from linear measurements under the assumption of sparsity in some basis. Recently, it has been shown that significantly fewer measurements may be required if the sparsity assumption is replaced by the assumption that the unknown vector lies near the range of a suitably-chosen generative model.  In particular, in (Bora {\em et al.}, 2017) it was shown that roughly $O(k\log L)$ random Gaussian measurements suffice for accurate recovery when the $k$-input generative model is bounded and $L$-Lipschitz, and that $O(kd \log w)$ measurements suffice for $k$-input ReLU networks with depth $d$ and width $w$.  In this paper, we establish corresponding algorithm-independent lower bounds on the sample complexity using tools from minimax statistical analysis.  In accordance with the above upper bounds, our results are summarized as follows: (i) We construct an $L$-Lipschitz generative model capable of generating group-sparse signals, and show that the resulting necessary number of measurements is $\Omega(k \log L)$; (ii) Using similar ideas, we construct two-layer ReLU networks of high width requiring $\Omega(k \log w)$ measurements, as well as lower-width deep ReLU networks requiring $\Omega(k d)$ measurements.  As a result, we establish that the scaling laws derived in (Bora {\em et al.}, 2017) are optimal or near-optimal in the absence of further assumptions.",https://openreview.net/pdf/fc3370ed0566d8ded92a02163be5259073319a72.pdf,stupidity,"['Zhaoqiang Liu', 'Jonathan Scarlett']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1l5s7298H,Learning-Based Low-Rank Approximations,"['low-rank approximation', 'learning-based algorithms', 'sketching/streaming algorithms']","We introduce a “learning-based” algorithm for the low-rank decomposition problem: given an $n \times d$ matrix $A$, and a parameter $k$, compute a rank-$k$ matrix $A'$ that minimizes the approximation loss $||A- A'||_F$. The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection $SA$, where $S$ is a sparse random $m \times n$ “sketching matrix”, and then performing the singular value decomposition of $SA$. We show how to replace the random matrix $S$ with a “learned” matrix of the same sparsity to reduce the error.

Our experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix $S$, sometimes by one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees.",https://openreview.net/pdf/52969344d31bb2c829a567fb96538c6a8194ec30.pdf,stupidity,"['Piotr Indyk', 'Ali Vakilian', 'Yang Yuan']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HylijQ35IS,Exploring Properties of the Deep Image Prior,"['deep learning', 'image reconstruction', 'adversarial examples', 'natural images']","The Deep Image Prior (DIP, Ulyanov et al., 2017) is a fascinating recent approach for recovering images which appear natural, yet is not fully understood. This work aims at shedding some further light on this approach by investigating the properties of the early outputs of the DIP. First, we show that these early iterations demonstrate invariance to adversarial perturbations by classifying progressive DIP outputs and using a novel saliency map approach. Next we explore using DIP as a defence against adversaries, showing good potential. Finally, we examine the adversarial invariancy of the early DIP outputs, and hypothesize that these outputs may remove non-robust image features. By comparing classification confidence values we show some evidence confirming this hypothesis.",https://openreview.net/pdf/dd07aa2c1418b292281e3d6fa5e2cc831ba78663.pdf,stupidity,"['Andreas Kattamis', 'Tameem Adel', 'Adrian Weller']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hyleh7hqUH,Generative Inpainting Network Applications on Seismic Image Compression and Non-Uniform Sampling,"['Compressive Sensing', 'GAN', 'WGAN', 'Inpainting', 'Non-Uniform sampling', 'Seismic acquisition', 'GoM']","The use of deep learning models as priors for compressive sensing tasks presents new potential for inexpensive seismic data acquisition. An appropriately designed Wasserstein generative adversarial network is designed based on a generative adversarial network architecture trained on several historical surveys, capable of learning the statistical properties of the seismic wavelets. The usage of validating and performance testing of compressive sensing are three steps. First, the existence of a sparse representation with different compression rates for seismic surveys is studied. Then, non-uniform samplings are studied, using the proposed methodology. Finally, recommendations for non-uniform seismic survey grid, based on the evaluation of reconstructed seismic images and metrics, is proposed. The primary goal of the proposed deep learning model is to provide the foundations of an optimal design for seismic acquisition, with less loss in imaging quality. Along these lines, a compressive sensing design of a non-uniform grid over an asset in Gulf of Mexico, versus a traditional seismic survey grid which collects data uniformly at every few feet, is suggested, leveraging the proposed method.",https://openreview.net/pdf/7a6aa038397d29e59a7d31f1ea731d7ce4cd35f0.pdf,stupidity,"['Xiaoyang Rebecca Li', 'Nikolaos Mitsakos', 'Ping Lu', 'Yuan Xiao', 'Xing Zhao']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hyet2Q29IS,Learned imaging with constraints and uncertainty quantification,"['Inverse problems', 'Seismic imaging', 'Langevin dynamics', 'Uncertainty quantification']","We outline new approaches to incorporate ideas from deep learning into wave-based least-squares imaging. The aim, and main contribution of this work, is the combination of handcrafted constraints with deep convolutional neural networks, as a way to harness their remarkable ease of generating natural images. The mathematical basis underlying our method is the expectation-maximization framework, where data are divided in batches and coupled to additional ""latent"" unknowns. These unknowns are pairs of elements from the original unknown space (but now coupled to a specific data batch) and network inputs. In this setting, the neural network controls the similarity between these additional parameters, acting as a ""center"" variable. The resulting problem amounts to a maximum-likelihood estimation of the network parameters when the augmented data model is marginalized over the latent variables.",https://openreview.net/pdf/2335ec0bfbaabfd671aee0dde1d77343a99adc52.pdf,stupidity,"['Felix J. Herrmann', 'Ali Siahkoohi', 'Gabrio Rizzuti']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HkxW672q8B,Learning Network Parameters in the ReLU Model,"['ReLU', 'Generative Model', 'One-Layer Network']","Rectified linear units, or ReLUs, have become a preferred activation function for artificial neural networks. In this paper we consider the problem of learning a generative model in the presence of nonlinearity (modeled by the ReLU functions). Given a set of signal vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we  aim to learn the network parameters, i.e., the $d\times k$ matrix $A$, under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i +\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias vector, and {$\mathbf{c}^i \in \mathbb{R}^k$ are arbitrary unknown latent vectors}. We show that it is possible to recover the column space of $A$ within an error of $O(d)$ (in Frobenius norm) under certain conditions on the  distribution of $\mathbf{b}$. ",https://openreview.net/pdf/5394a5ea64f2b2d2d39ba5a4d84c4448a80e00aa.pdf,stupidity,"['Arya Mazumdar', 'Ankit Singh Rawat']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJlL2Q2qLS,GAN priors for Bayesian inference,"['Bayesian inference', 'GAN priors', 'Uncertainty quantification']","Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a mathematical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to characterize mathematically. In this work we demonstrate how the approximate distribution learned by a generative adversarial network (GAN) may be used as a prior in a Bayesian update to address both these challenges. We demonstrate the efficacy of this approach by inferring and quantifying uncertainty in a physics-based inverse problem and an inverse problem arising in computer vision. In this latter example, we also demonstrate how the knowledge of the spatial variation of uncertainty may be used to select an optimal strategy of placing the sensors (i.e. taking measurements), where information about the image is revealed one sub-region at a time.",https://openreview.net/pdf/7d4f45c87f64915c561880b4fd5428c01b9ced86.pdf,stupidity,"['Dhruv V. Patel', 'Assad A. Oberai']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJgAjm3qLB,"Unrolled, model-based networks for lensless imaging","['computational imaging', 'mask-based lensless imaging', 'unrolled algorithms']","We develop end-to-end learned reconstructions for lensless mask-based cameras, including an experimental system for capturing aligned lensless and lensed images for training.  Various reconstruction methods are explored, on a scale from classic iterative approaches (based on the physical imaging model) to deep learned methods with many learned parameters.  In the middle ground, we present several variations of unrolled alternating direction method of multipliers (ADMM) with varying numbers of learned parameters. The network structure combines knowledge of the physical imaging model with learned parameters updated from the data, which compensate for artifacts caused by physical approximations. Our unrolled approach is 20X faster than classic methods and produces better reconstruction quality than both the classic and deep methods on our experimental system.  ",https://openreview.net/pdf/2a3f55026202c9aa56e4f705961f5f50fd23b6b4.pdf,stupidity,"['Kristina Monakhova', 'Joshua Yurtsever', 'Grace Kuo', 'Nick Antipa', 'Kyrollos Yanny', 'Laura Waller']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1x22Xn5Ur,Retrieving Signals with Deep Complex Extractors,"['Deep Complex Networks', 'Signal Extraction']","Recent advances have made it possible to create deep complex-valued neural networks. Despite this progress, many challenging learning tasks have yet to leverage the power of complex representations. Building on recent advances, we propose a new deep complex-valued method for signal retrieval and extraction in the frequency domain. As a case study, we perform audio source separation in the Fourier domain. Our new method takes advantage of the convolution theorem which states that the Fourier transform of two convolved signals is the elementwise product of their Fourier transforms. Our novel method is based on a complex-valued version of Feature-Wise Linear Modulation (FiLM) and serves as the keystone of our proposed signal extraction method. We also introduce a new and explicit amplitude and phase-aware loss, which is scale and time invariant, taking into account the complex-valued components of the spectrogram. Using the Wall Street Journal Dataset, we compared our phase-aware loss to several others that operate both in the time and frequency domains and demonstrate the effectiveness of our proposed signal extraction method and proposed loss.",https://openreview.net/pdf/0b69e0ebbed4e8561cb5886724a3cbf0976ddda0.pdf,stupidity,"['Chiheb Trabelsi', 'Olexa Bilaniuk', 'Ousmane Dia', 'Ying Zhang', 'Mirco Ravanelli', 'Jonathan Binas', 'Negar Rostamzadeh', 'Christopher  J Pal']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ByxLnmnqUr,$C^\infty$ Smooth Algorithmic Neural Networks for Solving Inverse Problems,"['Smoothness', 'Differentiable', 'Inverse Problems', 'Adversarial Training', 'Neural Networks', 'Deep Learning']","Artificial neural networks revolutionized many areas of computer science in recent years since they provide solutions to a number of previously unsolved problems.
On the other hand, for many problems, classic algorithms exist, which typically exceed the accuracy and stability of neural networks.
To combine these two concepts, we present a new kind of neural networks—algorithmic neural networks.
These networks integrate smooth versions of classic algorithms into the topology of neural networks.
Our novel reconstructive adversarial network (RAN) enables solving inverse problems without or with only weak supervision.",https://openreview.net/pdf/58d29d5448eb5a7b876983d5f11a22fc97507d52.pdf,stupidity,"['Felix Petersen', 'Christian Borgelt', 'Oliver Deussen']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BylRn72cUH,Unsupervised Deep Basis Pursuit: Learning inverse problems without ground-truth data,"['Unsupervised learning', 'inverse problems', 'model-based deep learning', 'computational imaging', 'magnetic resonance imaging']","Basis pursuit is a compressed sensing optimization in which the l1-norm is minimized subject to model error constraints. Here we use a deep neural network prior instead of l1-regularization. Using known noise statistics, we jointly learn the prior and reconstruct images without access to ground-truth data. During training, we use alternating minimization across an unrolled iterative network and jointly solve for the neural network weights and training set image reconstructions. At inference, we fix the weights and pass the measurements through the network. We compare reconstruction performance between unsupervised and supervised (i.e. with ground-truth) methods. We hypothesize this technique could be used to learn reconstruction when ground-truth data are unavailable, such as in high-resolution dynamic MRI.",https://openreview.net/pdf/cf32103a5f52b1fc0cc6f3bf6fedd3f0c0673782.pdf,stupidity,"['Jonathan I. Tamir', 'Stella X. Yu', 'Michael Lustig']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Bygi2739Lr,Y-net: A Physics-constrained and Semi-supervised Learning Approach to the Phase Problem in Computational Electron Imaging,"['phase retrieval', 'semi-supervised learning', 'inverse problem', 'phase problem']","The phase problem in diffraction physics is one of the oldest inverse problems in all of science. The central difficulty that any approach to solving this inverse problem must overcome is that half of the information, namely the phase of the diffracted beam, is always missing. In the context of electron microscopy, the phase problem is generally non-linear and solutions provided by phase-retrieval techniques are known to be poor approximations to the physics of electrons interacting with matter. Here, we show that a semi-supervised learning approach can effectively solve the phase problem in electron microscopy/scattering. In particular, we introduce a new Deep Neural Network (DNN), Y-net, which simultaneously learns a reconstruction algorithm via supervised training in addition to learning a physics-based regularization via unsupervised training. We demonstrate that this constrained, semi-supervised approach is an order of magnitude more data-efficient and accurate than the same model trained in a purely supervised fashion. In addition, the architecture of the Y-net model provides for a straightforward evaluation of the consistency of the model's prediction during inference and is generally applicable to the phase problem in other settings.",https://openreview.net/pdf/181d73aa6d87570f2b972d194782a1ae47017935.pdf,stupidity,"['Nouamane Laanait', 'Junqi Yin', 'Albina Borisevich']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BkxP2mnq8S,Lower Bounds for Compressed Sensing with Generative Models,"['lower bounds', 'compressed sensing', 'sparsity']","  The goal of compressed sensing is to learn a structured signal $x$
  from a limited number of noisy linear measurements $y \approx Ax$.  In
  traditional compressed sensing, ``structure'' is represented by
  sparsity in some known basis.  Inspired by the success of deep
  learning in modeling images, recent work starting with~\cite{BDJP17}
  has instead considered structure to come from a generative model
  $G: \R^k \to \R^n$.  We present two results establishing the
  difficulty of this latter task, showing that existing bounds are
  tight.

  First, we provide a lower bound matching the~\cite{BDJP17} upper
  bound for compressed sensing from $L$-Lipschitz generative models
  $G$.  In particular, there exists such a function that requires
  roughly $\Omega(k \log L)$ linear measurements for sparse recovery
  to be possible.  This holds even for the more relaxed goal of
  \emph{nonuniform} recovery.

  Second, we show that generative models generalize sparsity as a
  representation of structure.  In particular, we construct a
  ReLU-based neural network $G: \R^{2k} \to \R^n$ with $O(1)$ layers
  and $O(kn)$ activations per layer, such that the range of $G$
  contains all $k$-sparse vectors.
",https://openreview.net/pdf/78bf1a4d54b3d761ad3499ccd94aa99ef770121c.pdf,stupidity,"['Akshay Kamath', 'Sushrut Karmalkar', 'Eric Price']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BklhoQ258B,Compressed Sensing and Overparametrized Networks: Overfitting Peaks in a Model of Misparametrized Sparse Regression in the Interpolation Limit,"['compressed sensing', 'overfitting', 'perfect fitting', 'double descent', 'thermodynamic limit', 'interpolation']","Current practice in machine learning is to employ deep nets in an overparametrized limit, with the nominal number of parameters typically exceeding the number of measurements. This resembles the situation in compressed sensing, or in sparse regression with $l_1$ penalty terms, and provides a theoretical avenue for understanding phenomena that arise in the context of deep nets. One such phenonemon is the success of deep nets in providing good generalization in an interpolating regime with zero training error. Traditional statistical practice calls for regularization or smoothing to prevent ""overfitting"" (poor generalization performance). However, recent work shows that there exist data interpolation procedures which are statistically consistent and provide good generalization performance\cite{belkin2018overfitting} (""perfect fitting""). In this context, it has been suggested that ""classical"" and ""modern"" regimes for machine learning are separated by a peak in the generalization error (""risk"") curve, a phenomenon dubbed ""double descent""\cite{belkin2019reconciling}. While such overfitting peaks do exist and arise from ill-conditioned design matrices, here we challenge the interpretation of the overfitting peak as demarcating the regime where good generalization occurs under overparametrization. 

We propose a model of Misparamatrized Sparse Regression (MiSpaR) and analytically compute the GE curves for $l_2$ and $l_1$ penalties. We show that the overfitting peak arising in the interpolation limit is dissociated from the regime of good generalization. The analytical expressions are obtained in the so called ""thermodynamic"" limit. We find an additional interesting phenomenon: increasing overparametrization in the fitting model increases sparsity, which should intuitively improve performance of $l_1$ penalized regression. However, at the same time, the relative number of measurements decrease compared to the number of fitting parameters, and eventually overparametrization does lead to poor generalization. Nevertheless, $l_1$ penalized regression can show good generalization performance under conditions of data interpolation even with a large amount of overparametrization. These results provide a theoretical avenue into studying inverse problems in the interpolating regime using overparametrized fitting functions such as deep nets. ",https://openreview.net/pdf/174a8eec53bcba3b5a7d02f824521274cfd5a174.pdf,stupidity,['Partha P Mitra'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Bkec3m3q8B,Neural reparameterization improves structural optimization,"['deep image prior', 'untrained neural networks', 'structural optimization']","Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produced significantly better designs than baseline methods.",https://openreview.net/pdf/ad439b53f0423a7dde132d7e387bc5ed1a247207.pdf,stupidity,"['Stephan Hoyer', 'Jascha Sohl-Dickstein', 'Sam Greydanus']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJx3nXn9Ir,Auto-encoders for compressed sensing,[],"Compressed sensing is about recovering a structured high-dimensional signal  ${\bf x}\in R^n$ from its under-determined noisy linear measurements ${\bf y}\in R^m$, where $m\ll n$. While the vast majority of the literature in this area is on sparse signals, in recent years, there has been considerable progress on compressed sensing  of signals with structures beyond sparsity. One of the promising approaches in this field is to employ generative models that are based on trained neural networks. 
In this paper, we study the performance of an iterative algorithm  based on projected gradient descent that employs an auto-encoder to define and enforce the source structure. The auto-encoder is defined by a generative function $g:R^k\rightarrow R^n$ and a separate neural network that is trained to function as the inverse of $g$. We prove that, for a generative model $g$ with $\ell_2$ representation error $\delta$,  given roughly $m>40k\log{1\over \delta}$ measurements, such an algorithm converges, even in the presence of additive white Gaussian noise. ",https://openreview.net/pdf/514309169e16dac86f4a70ae9751f1e2253afdfb.pdf,stupidity,"['Pei Peng', 'Shirin Jalali', 'Xin Yuan']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJlD2X3qUB,PatchDIP Exploiting Patch Redundancy in Deep Image Prior for Denoising,"['deep image prior', 'image denoising', 'patch recurrence']","The structure of a deep convolutional neural network initialized with random weights is able to sufficiently capture the patterns in a natural image. This finding motivates using deep neural network as an effective prior for natural images. In this work, we show that this strong prior, enforced by the structure of a ConvNet, can be augmented with the information that recurs in different patches of a natural image to boost the performance. We demonstrate that the self-similarity in the image patches can be exploited alongside deep image prior by optimizing the network weights to fit patches extracted from a single noisy image. Our results indicate that employing deep image prior on noisy patches provides an additional disincentive for the network to fit noise, and is encouraged to exploit redundancies among the patches yielding better denoising performance.
",https://openreview.net/pdf/f09b569521106a780d200da6adc4106e81cdf8a0.pdf,stupidity,"['Muhammad Asim', 'Fahad Shamshad', 'Ali Ahmed']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJgmnmn5Lr,Generative Models for Low-Dimensional Video Representation and Compressive Sensing,"['generative network', 'deep image prior', 'nonlinear embedding']","Generative priors have become highly effective in solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. With a generative model we can represent an image with a much lower dimensional latent codes. In the context of compressive sensing, if the unknown image belongs to the range of a pretrained generative network, then we can recover the image by estimating the underlying compact latent code from the available measurements. However, recent studies revealed that even untrained deep neural networks can work as a prior for recovering natural images. These approaches update the network weights keeping latent codes fixed to reconstruct the target image from the given measurements. In this paper, we optimize over network weights and latent codes to use untrained generative network as prior for video compressive sensing problem. We show that by optimizing over latent code, we can additionally get concise representation of the frames which retain the structural similarity of the video frames. We also apply low-rank constraint on the latent codes to represent the video sequences in even lower dimensional latent space. We empirically show that our proposed methods provide better or comparable accuracy and low computational complexity compared to the existing methods.",https://openreview.net/pdf/726c1327a45b5d7c357969ba09505e4a08b8e833.pdf,stupidity,"['Rakib Hyder', 'M. Salman Asif']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1xsi725IB,A Hybrid Architecture for On-Device Compressive Machine Learning,"['Deep Learning', 'Compressive Sensing', 'Machine Learning', 'Internet of Things']","Developing machine learning techniques that can protect the privacy of users’ data is of utmost importance, as tracking and selling our digital data (often without our permission and knowledge) has become a booming business model. By processing the data on the device that has collected the data, we can dramatically increase the level of privacy. On-device machine offers several additional benefits, such as low latency, efficient use of network bandwidth, and more autonomy. However, many devices deployed on the “edge” have very limited memory, weak processors, and scarce energy supply. This poses the challenge of envisioning new machine learning architectures that can function properly under such dire conditions. This issue is particularly urgent with the emergence of the Internet of Things.

We propose a hybrid hardware-software framework that facilitates increased privacy protection due to on-device processing and moreover has the potential to significantly reduce the computational complexity and memory requirements of on-device machine learning. In the first step, inspired by compressive sensing, data is collected in compressed form simultaneously with the sensing process. Thus this compression happens already at the hardware level during data acquisition. But unlike in compressive sensing, this compression is achieved via a projection operator that is specifically tailored to the desired machine learning task. The second step consists of a specially designed and trained deep network. Numerical simulations in image classification illustrate the viability of our method.",https://openreview.net/pdf/8baf7035009b5713a617569f5221c494e4782d65.pdf,stupidity,"['Yang Li', 'Thomas Strohmer']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1gznX3q8S,Improving Limited Angle CT Reconstruction with a Robust GAN Prior,"['GANs', 'imaging priors', 'CT reconstruction', 'robustness', 'limited angle CT']","Limited angle CT reconstruction is an under-determined linear inverse problem that requires appropriate regularization techniques to be solved. In this work we study how pre-trained generative adversarial networks (GANs) can be used to clean noisy, highly artifact laden reconstructions from conventional techniques, by effectively projecting onto the inferred image manifold. In particular, we use a robust version of the popularly used GAN prior for inverse problems, based on a recent technique called corruption mimicking, that significantly improves the reconstruction quality. The proposed approach operates in the image space directly, as a result of which it does not need to be trained or require access to the measurement model, is scanner agnostic, and can work over a wide range of sensing scenarios.",https://openreview.net/pdf/be317490fa79ccf76ee3ed77e94c56620189486c.pdf,stupidity,"['Rushil Anirudh', 'Hyojin Kim', 'Jayaraman J. Thiagarajan', 'K. Aditya Mohan', 'Kyle Champley']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1g-h7398H,Extreme Few-view CT Reconstruction using Deep Inference,"['Computed Tomography', 'Few-View CT', 'CNN']","Reconstruction of few-view x-ray Computed Tomography (CT) data is a highly ill-posed problem. It is often used in applications that require low radiation dose in clinical CT, rapid industrial scanning, or fixed-gantry CT. Existing analytic or iterative algorithms generally produce poorly reconstructed images, severely deteriorated by artifacts and noise, especially when the number of x-ray projections is considerably low. This paper presents a deep network-driven approach to address extreme few-view CT by incorporating convolutional neural network-based inference into state-of-the-art iterative reconstruction. The proposed method interprets few-view sinogram data using attention-based deep networks to infer the reconstructed image. The predicted image is then used as prior knowledge in the iterative algorithm for final reconstruction. We demonstrate effectiveness of the proposed approach by performing reconstruction experiments on a chest CT dataset. ",https://openreview.net/pdf/41cafda1008d3269d9e6c1eaaf6559188dd3d670.pdf,stupidity,"['Hyojin Kim', 'Rushil Anirudh', 'K. Aditya Mohan', 'Kyle Champley']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkeRMT9cLH,BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding,"['BERT', 'Fine Tuning', 'Legal', 'Documents', 'Contracts', 'Understanding']","Fine-tuning language models, such as BERT, on domain specific corpora has proven to be valuable in domains like scientific papers and biomedical text. In this paper, we show that fine-tuning BERT on legal documents similarly provides valuable improvements on NLP tasks in the legal domain. Demonstrating this outcome is significant for analyzing commercial agreements, because obtaining large legal corpora is challenging due to their confidential nature. As such, we show that having access to large legal corpora is a competitive advantage for commercial applications, and academic research on analyzing contracts.",https://openreview.net/pdf/6005eece2c0d36c860e1b2888e0bdb1b3e9cd0c0.pdf,stupidity,"['Emad Elwany', 'Dave Moore', 'Gaurav Oberoi']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1x3GTq5IB,Semantic Structure Extraction for Spreadsheet Tables with a Multi-task Learning Architecture,"['spreadsheet', 'table', 'deep learning', 'semantic']","Semantic structure extraction for spreadsheets includes detecting table regions, recognizing structural components and classifying cell types. Automatic semantic structure extraction is key to automatic data transformation from various table structures into canonical schema so as to enable data analysis and knowledge discovery. However, they are challenged by the diverse table structures and the spatial-correlated semantics on cell grids. To learn spatial correlations and capture semantics on spreadsheets, we have developed a novel learning-based framework for spreadsheet semantic structure extraction. First, we propose a multi-task framework that learns table region, structural components and cell types jointly; second, we leverage the advances of the recent language model to capture semantics in each cell value; third, we build a large human-labeled dataset with broad coverage of table structures. Our evaluation shows that our proposed multi-task framework is highly effective that outperforms the results of training each task separately.",https://openreview.net/pdf/df75806fbf652d23820e44d83196ade78b1ac6e9.pdf,stupidity,"['Haoyu Dong', 'Shijie Liu', 'Zhouyu Fu', 'Shi Han', 'Dongmei Zhang']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SkxhzT5qIB,Chargrid-OCR: End-to-end trainable Optical Character Recognition through Semantic Segmentation and Object Detection,"['OCR', 'Computer Vision', 'Tesseract', 'Printed documents', 'Document Intelligence']","We present an end-to-end trainable approach for optical character recognition (OCR) on printed documents. It is based on predicting a two-dimensional character grid ('chargrid') representation of a document image as a semantic segmentation task.
To identify individual character instances from the chargrid, we regard characters as objects and use object detection techniques from computer vision.
We demonstrate experimentally that our method outperforms previous state-of-the-art approaches in accuracy while being easily parallelizable on GPU (thereby being significantly faster), as well as easier to train.",https://openreview.net/pdf/40321f7bfc604d7a71c29b1c6406e156c512fd5f.pdf,stupidity,"['Christian Reisswig', 'Anoop R Katti', 'Marco Spinaci', 'Johannes Höhne']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Ske6GT9c8r,On recognition of Cyrillic Text,"['Computer Vision', 'Optical Character Recognition', 'Transfer Learning', 'Datasets', 'Handwritten Text Recognition', 'Document Intelligence', 'Cyrillic Text']","We introduce the largest (among publicly available) dataset for Cyrillic Handwritten Text Recognition and the first dataset for Cyrillic Text in the Wild Recognition, as well as suggest a method for recognizing Cyrillic Handwritten Text and Text in the Wild. Based on this approach, we develop a system that can reduce the document processing time for one of the largest mathematical competitions in Ukraine by 12 days and the amount of used paper by 0.5 ton.",https://openreview.net/pdf/fc045b4a24ba64805b8273c0f83fffec81149f9e.pdf,stupidity,"['Kostiantyn Liepieshov', 'Oles Dobosevych']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJl3z659UH,CORD: A Consolidated Receipt Dataset for Post-OCR Parsing,"['Receipt', 'Dataset', 'Semantic Parsing', 'OCR', 'post-OCR']","OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks.",https://openreview.net/pdf/aff75f82ef7d5bfdacbea29ada9e0912551e5364.pdf,stupidity,"['Seunghyun Park', 'Seung Shin', 'Bado Lee', 'Junyeop Lee', 'Jaeheung Surh', 'Minjoon Seo', 'Hwalsuk Lee']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJgjf695UB,Post-OCR parsing: building simple and robust parser via BIO tagging,"['Post-OCR', 'parsing', 'OCR', 'tagging', 'BIO']","Parsing textual information embedded in images is important for various down- stream tasks. However, many previously developed parsers are limited to handling the information presented in one dimensional sequence format. Here, we present Post Ocr Tagging based parser (POT), a simple and robust parser that can parse visually embedded texts by BIO-tagging the output of optical character recognition (OCR) task. Our shallow parsing approach enables building robust neural parser with less than a thousand labeled data. POT is validated on receipt and namecard parsing tasks.",https://openreview.net/pdf/eb1274f0d65b4c3e30e951b4668b7f982338a628.pdf,stupidity,"['Wonseok Hwang', 'Seonghyeon Kim', 'Minjoon Seo', 'Jinyeong Yim', 'Seunghyun Park', 'Sungrae Park', 'Junyeop Lee', 'Bado Lee', 'Hwalsuk Lee']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJgazaq5Ir,Representation Learning in Geology and GilBERT,"['representation learning', 'word2vec', 'universal sentence encoder', 'bert', 'geological embeddings', 'analogs']","Geology lays at the foundation of the oil and gas industry and a good understanding of geology in each newly drilled well can make or break an exploration project with a price tag in the millions of dollars.  Over the past decades, each drilled well have been extensively analyzed, where geology and other petrophysical properties were interpreted by experts and rigorously documented. As this creates a valuable source of information for future drilling success, most of it is stored in PDF files in knowledge silos of companies. Recent advancements in cloud technologies and machine learning techniques are enabling the future to be open-source and access to these technical documents is providing a broad geological knowledge of the different basins in the world.  In this work, we focus on geology reports of wells drilled in the Norwegian Sea with the goal to learn numerical representations for geological descriptions in these fields and utilize these representations to find worldwide geological analogues. The automation of analog identification can improve expert interpretation, exploration success, and save a significant amount of effort and time for oil and gas companies. We will present numerical encoding approaches we took in the pursuit of capturing representations of geological knowledge from files as well as challenges faced during this work and road map towards GilBERT; Geologically informed language modeling with BERT, for the use in geology-based NLP applications in oil-and-gas (O&G) industry.",https://openreview.net/pdf/eab0208dd979b944dd8ef95642d462b58eeb5125.pdf,stupidity,"['Zikri Bayraktar', 'Hedi Driss', 'Marie Lefranc']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJgRf659Ur,DeepErase: Weakly Supervised Ink Artifact Removal in Document Text Images,"['document analysis', 'semantic segmentation', 'computer vision']","Still in 2019, many scanned documents come into businesses in non-digital format. Text to be extracted from real world documents is often nestled inside rich formatting, such as tabular structures or forms with fill-in-the-blank boxes or underlines whose ink often touches or even strikes through the ink of the text itself. Such ink artifacts can severely interfere with the performance of recognition algorithms or other downstream processing tasks. In this work, we propose DeepErase, a neural preprocessor to erase ink artifacts from text images. We devise a method to programmatically augment text images with real artifacts, and use them to train a segmentation network in an weakly supervised manner. In additional to high segmentation accuracy, we show that our cleansed images achieve a significant boost in downstream recognition accuracy by popular OCR software such as Tesseract 4.0. We test DeepErase on out-of-distribution datasets (NIST SDB) of scanned IRS tax return forms and achieve double-digit improvements in recognition accuracy over baseline for both printed and handwritten text.",https://openreview.net/pdf/dd6ab3b2126cdc5d3401811a99545ec01cfe1990.pdf,stupidity,"['Yike Qi', 'W. Ronny Huang', 'Qianqian Li', 'Jonathan L. Degange']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1xkQac9LB,Towards Neural Similarity Evaluator,[],"We review three limitations of BLEU and ROUGE – the most popular metrics
used to assess reference summaries against hypothesis summaries, come up with
criteria for what a good metric should behave like and propose concrete ways to
assess the performance of a metric in detail and show the potential of Transformers-based Language Models to assess reference summaries against hypothesis summaries.",https://openreview.net/pdf/762b23f00d543ac6e92fd249aec84ff3fcd37522.pdf,stupidity,"['Hassan Kané', 'Yusuf Kocyigit', 'Pelkins Ajanoh', 'Ali Abdalla', 'Mohamed Coulibali']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1eTMp59LB,Doc2Dial: a Framework for Dialogue Composition Grounded in Business Documents,"['business documents', 'dialog', 'data set']"," We introduce Doc2Dial, an end-to-end framework for generating conversational data grounded in business documents via crowdsourcing. Such data can be used to train automated dialogue agents performing customer care tasks for the enterprises or organizations. In particular, the framework takes the documents as input and generates the tasks for obtaining the annotations for simulating dialog flows. The dialog flows are used to guide the collection of utterances produced by crowd workers. The outcomes include dialogue data grounded in the given documents, as well as various types of annotations that help ensure the quality of the data and the flexibility to (re)composite dialogues.",https://openreview.net/pdf/d69afd416876b8d1c57a03df733006fcd34b7fed.pdf,stupidity,"['Song Feng', 'Kshitij Fadni', 'Q. Vera Liao', 'Luis A. Lastras']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1Mnzp9qLB,Document Enhancement System Using Auto-encoders,"['Document Enhancement', 'document image cleanup', 'Deep Neural Networks', 'Watermark Removal', 'ResNets', 'Skip Connections']",The conversion of scanned documents to digital forms is performed using an Optical Character Recognition (OCR) software. This work focuses on improving the quality of scanned documents in order to improve the OCR output. We create an end-to-end document enhancement pipeline which takes in a set of noisy documents and produces clean ones. Deep neural network based denoising auto-encoders are trained to improve the OCR quality. We train a blind model that works on different noise levels of scanned text documents. Results are shown for blurring and watermark noise removal from noisy scanned documents.,https://openreview.net/pdf/9bebab53671ff990f726065d3d10090924ac8e79.pdf,stupidity,"['Mehrdad J. Gangeh', 'Sunil R. Tiyyagura', 'Sridhar V. Dasaratha', 'Hamid Motahari', 'Nigel P. Duffy']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hyx3f65qLS,SVDocNet: Spatially Variant U-Net for Blind Document Deblurring,"['Blind Document Deblurring', 'Neural Networks', 'Deep Learning']","Blind document deblurring is a fundamental task in the field of document processing and restoration, having wide enhancement applications in optical character recognition systems, forensics, etc. Since this problem is highly ill-posed, supervised and unsupervised learning methods are well suited for this application. Using various techniques, extensive work has been done on natural-scene deblurring. However, these extracted features are not suitable for document images. We present SVDocNet, an end-to-end trainable U-Net based spatial recurrent neural network (RNN) for blind document deblurring where the weights of the RNNs are determined by different convolutional neural networks (CNNs). This network achieves state of the art performance in terms of both quantitative measures and qualitative results.",https://openreview.net/pdf/165bbb9e99aabee618001c795ccb5cc65afce903.pdf,stupidity,"['Bharat Mamidibathula', 'Prabir Kumar Biswas']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hkx0zpccLr,Information Extraction from Text Regions with Complex Tabular Structure,[],"Recent innovations have improved layout analysis of document images, significantly improving our ability to identify text and non-text regions. However, extracting information from within text regions remains quite challenging because the text region may have a complex structure. In this paper, we present a new dataset with complex text structure, and propose new methods to robustly retrieve information from the complex text region.",https://openreview.net/pdf/100491eb27a4c2a5f91687b4b140dd8f2a0d40f2.pdf,stupidity,"['Kaixuan Zhang', 'Zejiang Shen', 'Jie Zhou', 'Melissa Dell']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1gsGaq9US,BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding,"['document representation', 'contextualized embedding', 'information extraction from documents', 'tabulated data recognition and extraction', 'document intelligence']","For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction.",https://openreview.net/pdf/e57e1023afebbc1a72ceba87859aa523c66ceaa7.pdf,stupidity,"['Timo I. Denk', 'Christian Reisswig']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BkxiG6qqIr,CrossLang: the system of cross-lingual plagiarism detection,"['cross-lingual plagiarism detection', 'paraphrase detection']","Plagiarism and text reuse become more available with the Internet development. Therefore it is important to check scientific papers for the fact of cheating, especially in Academia. Existing systems of plagiarism detection show the good performance and have a huge source databases. Thus now it is not enough just to copy the text as is from the source document to get the original work. Therefore, another type of plagiarism become popular -- cross-lingual plagiarism. We present a CrossLang system for such kind of plagiarism detection for English-Russian language pair.",https://openreview.net/pdf/2674947f04ea7b4bbfe9c6851f4f13c1e6f92d0e.pdf,stupidity,"['Oleg Bakhteev', 'Alexandr Ogaltsov', 'Andrey Khazov', 'Kamil Safin', 'Rita Kuznetsova']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJx5zpc58r,Repurposing Decoder-Transformer Language Models for Abstractive Summarization,"['summarization', 'language models', 'attention', 'transformers']","Neural network models have shown excellent fluency and performance when applied to abstractive summarization. Many approaches to neural abstractive summarization involve the introduction of significant inductive bias, such as pointer-generator architectures, coverage, and partially extractive procedures, designed to mimic human summarization. We show that it is possible to attain competitive performance by instead directly viewing summarization as language modeling. We introduce a simple procedure built upon pre-trained decoder-transformers to obtain competitive ROUGE scores using a language modeling loss alone, with no beam-search or other decoding-time optimization, and instead rely on efficient nucleus sampling and greedy decoding.",https://openreview.net/pdf/5b123de11c7a61ba1a980ffe1ac6cc20ba06aeaa.pdf,stupidity,"['Luke de Oliveira', 'Alfredo Láinez Rodrigo']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJgAGp5qLH,On Domain Transfer When Predicting Intent in Text,"['text classification', 'transfer learning', 'enterprise text']","In many domains, especially enterprise text analysis, there is an abundance of data which can be used for the development of new AI-powered intelligent experiences to improve people's productivity. However, there are strong-guarantees of privacy which prevent broad sampling and  labeling of personal text data to learn or evaluate models of interest. Fortunately, in some cases like enterprise email, manual annotation is possible on certain public datasets. The hope is that models trained on these public datasets would perform well on the target private datasets of interest. In this paper, we study the challenges of transferring information from one email dataset to another, for predicting user intent. In particular, we present approaches to characterizing the transfer gap in text corpora from both an intrinsic and extrinsic point-of-view, and evaluate several proposed methods in the literature for bridging this gap. We conclude with raising issues for further discussion in this arena.",https://openreview.net/pdf/bbbc48b7282d0910bae11ff9ad4a38e6d3c2d9e1.pdf,stupidity,"['Petar Stojanov', 'Ahmed Hassan Awadallah', 'Paul Bennett', 'Saghar Hosseini']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1x6fa95UH,Neural Contract Element Extraction Revisited,"['contract element extraction', 'sequence labeling']","We investigate contract element extraction. We show that LSTM-based encoders perform better than dilated CNNs, Transformers, and BERT in this task. We also find that domain-specific WORD2VEC embeddings outperform generic pre-trained GLOVE embeddings. Morpho-syntactic features in the form of POS tag and token shape embeddings, as well as context-aware ELMO embeddings do not improve performance. Several of these observations contradict choices or findings of previous work on contract element extraction and generic sequence labeling tasks, indicating that contract element extraction requires careful task-specific choices.",https://openreview.net/pdf/96ea7d6b6746a1fcc6c6b76287a631fa7f65ae2f.pdf,stupidity,"['Ilias Chalkidis', 'Manos Fergadiotis', 'Prodromos Malakasiotis', 'Ion Androutsopoulos']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1eozp5c8r,From Stroke to Finite Automata: An Offline Recognition Approach,[]," A major challenge in making online education easier and more effective lies in developing automatic recognition, interpretation, and grading systems that can provide meaningful feedback to lecturers and students. Formal Languages and Automata Theory is a major module for many computer science and computing programmes worldwide. In such a module, students are taught how to design a finite state machine to recognise words in any given language. Despite the wide acceptance of this module by most universities across the globe, most students find this module difficult, boring and too abstract. Several research has been conducted on how to make this module interesting, but there still exist some gap. In this work, we propose and implement a system that assistant learners in learning this module. The system is in two units: unit one focus on offline recognition of hand-drawn finite automata diagram and the second unit focus on the tutor system. The system produced a 97% recognition rate. In future work, we intend to use formal grammars (second unit) to represent the recognised FA components, this will be used to automatically parse the output of the recognition system to determine if valid FA has been drawn.",https://openreview.net/pdf/38ef439ea57a6254d205434021b7ce282b430299.pdf,stupidity,['Kehinde Aruleba'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rylt7mFU8S,Additive function approximation in the brain,"['sparse networks', 'random features', 'associative learning']","Many biological learning systems such as the mushroom body, hippocampus, and cerebellum are built from sparsely connected networks of neurons. For a new understanding of such networks, we study the function spaces induced by sparse random features and characterize what functions may and may not be learned. A network with d inputs per neuron is found to be equivalent to an additive model of order d, whereas with a degree distribution the network combines additive terms of different orders. We identify three specific advantages of sparsity: additive function approximation is a powerful inductive bias that limits the curse of dimensionality, sparse networks are stable to outlier noise in the inputs, and sparse random features are scalable. Thus, even simple brain architectures can be powerful function approximators. Finally, we hope that this work helps popularize kernel theories of networks among computational neuroscientists.",https://openreview.net/pdf/59a2078935118b0acaf69428b3aed8622c45ca5f.pdf,stupidity,['Kameron Decker Harris'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rylq7mF8IS,Local Unsupervised Learning for Image Analysis,"['hebbian learning', 'local learning', 'orientation selectivity']","We use a recently proposed biologically plausible local unsupervised training algorithm (Krotov & Hopfield, PNAS 2019) for learning convolutional filters from CIFAR-10 images. These filters combined with patch normalization and very steep non-linearities result in a good classification accuracy for shallow networks trained locally, as opposed to end-to-end. The filters learned by our algorithm contain both orientation selective units and unoriented color units, resembling the responses of pyramidal neurons located in the cytochrome oxidase  “interblob” and “blob” regions in the primary visual cortex of primates. It is  shown that convolutional networks with patch normalization significantly outperform standard convolutional networks on the task of recovering the original classes when shadows are superimposed on top of standard CIFAR-10 images. Patch normalization approximates the retinal adaptation to the mean light intensity, important for human vision. All these results taken together suggest a possibility that local unsupervised training might be a useful tool for learning general representations (without specifying the task) directly from unlabeled data. ",https://openreview.net/pdf/e319631c897864ca179b60d761f9b469b10e505d.pdf,stupidity,"['Leopold Grinberg', 'John Hopfield', 'Dmitry Krotov']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rylU4mtUIS,Brain-inspired Robust Vision using Convolutional Neural Networks with Feedback,"['generative models', 'brain-inspired', 'robust vision']","Humans have the remarkable ability to correctly classify images despite possible degradation. Many studies have suggested that this hallmark of human vision results from the interaction between feedforward signals from bottom-up pathways of the visual cortex and feedback signals provided by top-down pathways. Motivated by such interaction, we propose a new neuro-inspired model, namely Convolutional Neural Networks with Feedback (CNN-F). CNN-F extends CNN with a feedback generative network, combining bottom-up and top-down inference to perform approximate loopy belief propagation.  We show that CNN-F's iterative inference allows for disentanglement of latent variables across layers. We validate the advantages of CNN-F over the baseline CNN. Our experimental results suggest that the CNN-F is more robust to image degradation such as pixel noise, occlusion, and blur.  Furthermore, we show that the CNN-F is capable of restoring original images from the degraded ones with high reconstruction accuracy while introducing negligible artifacts.",https://openreview.net/pdf/727e14d93f702e3dc3ad6a65041a132921f4241d.pdf,stupidity,"['Yujia Huang', 'Sihui Dai', 'Tan Nguyen', 'Pinglei Bao', 'Doris Y. Tsao', 'Richard G. Baraniuk', 'Anima Anandkumar']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rygpmmF8IS,EEG based Emotion Recognition of Image Stimuli ,"['Electroencephalography (EEG)', 'Brain computer interface (BCI)', 'machine learning', 'emotion recognition', 'image stimuli', 'neuromarketingg']","Emotion is playing a great role in our daily lives. The necessity and importance of an automatic Emotion recognition system is getting increased. Traditional approaches of emotion recognition are based on facial images, measurements of heart rates, blood pressure, temperatures, tones of voice/speech, etc. However, these features can potentially be changed to fake features. So to detect hidden and real features that is not controlled by the person are data measured from brain signals. There are various ways of measuring brain waves: EEG, MEG, FMRI, etc. On the bases of cost effectiveness and performance trade-offs, EEG is chosen for emotion recognition in this work. The main aim of this study is to detect emotion based on EEG signal analysis recorded from brain in response to visual stimuli. The approaches used were the selected visual stimuli were presented to 11 healthy target subjects and EEG signal were recorded in controlled situation to minimize artefacts (muscle or/and eye movements).  The signals were filtered and type of frequency band was computed and detected. The proposed method predicts an emotion type (positive/negative) in response to the presented stimuli. Finally, the performance of the proposed approach was tested. The average accuracy of machine learning algorithms (i.e. J48, Bayes Net, Adaboost and Random Forest) are 78.86, 74.76, 77.82 and 82.46 respectively.  In this study, we also applied EEG applications in the context of neuro-marketing. The results empirically demonstrated detection of the favourite colour preference of customers in response to the logo colour of an organization or Service. ",https://openreview.net/pdf/32a4e41264f35f6d23410e72eb54f967d8ffb63a.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryeT47FIIS,Tracking momentary attention fluctuations with an EEG-based cognitive brain-machine interface,"['BCI', 'attention', 'dimensionality reduction', 'subspaces', 'EEG', 'SSVEP', 'DSS']","Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention’s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.
",https://openreview.net/pdf/9dd8a09ad95615cede947c85b2668adf5f5c92fd.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryGcXQK8LS,"Convolutionary, Evolutionary, Revolutionary: What’s next for Bodies, Brains and AI?","['spiking neural networks', 'self organization', 'feedback', 'oscillations', 'predictive models', 'dynamic correlations', 'stochastic sampling', 'RL', 'embodiment']","In recent years we have made significant progress identifying computational principles that underlie neural function. While not yet complete, we have sufficient evidence that a synthesis of these ideas could result in an understanding of how neural computation emerges from a combination of innate dynamics and plasticity, and which could potentially be used to construct new AI technologies with unique capabilities. I discuss the relevant principles, the advantages they have for computation, and how they can benefit AI. Limitations of current AI are generally recognized, but fewer people are aware that we understand enough about the brain to immediately offer novel AI formulations.
",https://openreview.net/pdf/81961b552ffd075f495def704338de9024daab39.pdf,stupidity,['Peter Stratton'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkxcXmtUUS,How well do deep neural networks trained on object recognition characterize the mouse visual system?,"['mouse visual cortex', 'goal-driven modeling', 'object recognition', 'deep neural networks', 'hierarchical organization']","Recent work on modeling neural responses in the primate visual system has benefited from deep neural networks trained on large-scale object recognition, and found a hierarchical correspondence between layers of the artificial neural network and brain areas along the ventral visual stream. However, we neither know whether such task-optimized networks enable equally good models of the rodent visual system, nor if a similar hierarchical correspondence exists. Here, we address these questions in the mouse visual system by extracting features at several layers of a convolutional neural network (CNN) trained on ImageNet to predict the responses of thousands of neurons in four visual areas (V1, LM, AL, RL) to natural images. We found that the CNN features outperform classical subunit energy models, but found no evidence for an order of the areas we recorded via a correspondence to the hierarchy of CNN layers. Moreover, the same CNN but with random weights provided an equivalently useful feature space for predicting neural responses. Our results suggest that object recognition as a high-level task does not provide more discriminative features to characterize the mouse visual system than a random network. Unlike in the primate, training on ethologically relevant visually guided behaviors -- beyond static object recognition -- may be needed to unveil the functional organization of the mouse visual cortex. ",https://openreview.net/pdf/4f92bf2cd1712f7763200ae97c11cc37ac9e7e77.pdf,stupidity,"['Santiago A. Cadena', 'Fabian H. Sinz', 'Taliah Muhammad', 'Emmanouil Froudarakis', 'Erick Cobos', 'Edgar Y. Walker', 'Jake Reimer', 'Matthias Bethge', 'Andreas Tolias', 'Alexander S. Ecker']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkxSEQtLUS,Convolutional neural networks with extra-classical receptive fields,"['lateral connections', 'convolutional neural networks', 'extra-classical receptive fields', 'mouse V1', 'supervised and unsupervised learning']","In the visual system, neurons respond to a patch of the input known as their classical receptive field (RF), and can be modulated by stimuli in the surround. These interactions are often mediated by lateral connections, giving rise to extra-classical RFs. We use supervised learning via backpropagation to learn feedforward connections, combined with an unsupervised learning rule to learn lateral connections between units within a convolutional neural network. These connections allow each unit to integrate information from its surround, generating extra-classical receptive fields for the units in our new proposed model (CNNEx). We demonstrate that these connections make the network more robust and achieve better performance on noisy versions of the MNIST and CIFAR-10 datasets. Although the image statistics of MNIST and CIFAR-10 differ greatly, the same unsupervised learning rule generalized to both datasets. Our framework can potentially be applied to networks trained on other tasks, with the learned lateral connections aiding the computations implemented by feedforward connections when the input is unreliable.",https://openreview.net/pdf/2b80fbab78d45d4f76ac03c90d3fd712decbd3dd.pdf,stupidity,"['Brian Hu', 'Ramakrishnan Iyer', 'Stefan Mihalas']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkldVXKU8H,Foveated Downsampling Techniques,"['foveation', 'fovea', 'neuroscience', 'neural', 'network', 'downsampling', 'saliency', 'perception', 'brain']","Foveation is an important part of human vision, and a number of deep networks have also used foveation. However, there have been few systematic comparisons between foveating and non-foveating deep networks, and between different variable-resolution downsampling methods. Here we define several such methods, and compare their performance on ImageNet recognition with a Densenet-121 network. The best variable-resolution method slightly outperforms uniform downsampling. Thus in our experiments, foveation does not substantially help or hinder object recognition in deep networks. ",https://openreview.net/pdf/293c456d59f67f31484bc1dac4298d6e710abf4a.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJl_7mtULB,Emergent Structures and Lifetime Structure Evolution in Artificial Neural Networks,"['emergent networks', 'structure evolution', 'architecture search']","Motivated by the flexibility of biological neural networks whose connectivity structure changes significantly during their lifetime,we introduce the Unrestricted Recursive Network (URN) and demonstrate that it can exhibit similar flexibility during training via gradient descent. We show empirically that many of the different neural network structures commonly used in practice today (including fully connected, locally connected and residual networks of differ-ent depths and widths) can emerge dynamically from the same URN.These different structures can be derived using gradient descent on a single general loss function where the structure of the data and the relative strengths of various regulator terms determine the structure of the emergent network. We show that this loss function and the regulators arise naturally when considering the symmetries of the network as well as the geometric properties of the input data.",https://openreview.net/pdf/8dfdabf50276ac000a2ceb9f2ea149ae81beb239.pdf,stupidity,['Siavash Golkar'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJeT47F8Lr,Unravelling the neural signatures of dream recall in EEG: a deep learning approach,"['CNN', 'EEG', 'sleep', 'dreamers', 'tSNE', 'guided-backpropagation']","Dreams and our ability to recall them are among the most puzzling questions in sleep research. Specifically, putative differences in brain network dynamics between individuals with high versus low dream recall rates, are still poorly understood. In this study, we addressed this question as a classification problem where we applied deep convolutional networks (CNN) to sleep EEG recordings to predict whether subjects belonged to the high or low dream recall group (HDR and LDR resp.). Our model achieves significant accuracy levels across all the sleep stages, thereby indicating subtle signatures of dream recall in the sleep microstructure. We also visualized the feature space to inspect the subject-specificity of the learned features, thus ensuring that the network captured population level differences. Beyond being the first study to apply deep learning to sleep EEG in order to classify HDR and LDR, guided backpropagation allowed us to visualize the most discriminant features in each sleep stage. The significance of these findings and future directions are discussed.",https://openreview.net/pdf/03f3ed9278bf3c3e3709153850cce5c8acf02d70.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1lwNXKU8r,The Natural Tendency of Feed Forward Neural Networks to Favor Invariant Units,"['deep networks', 'invariance', 'neuroscience']","A central goal in the study of the primate visual cortex and hierarchical models for object recognition is understanding how and why single units trade off invariance versus sensitivity to image transformations. For example, in both deep networks and visual cortex there is substantial variation from layer-to-layer and unit-to-unit in the degree of translation invariance. Here, we provide theoretical insight into this variation and its consequences for encoding in a deep network. Our critical insight comes from the fact that rectification simultaneously decreases response variance and correlation across responses to transformed stimuli, naturally inducing a positive relationship between invariance and dynamic range. Invariant input units then tend to drive the network more than those sensitive to small image transformations. We discuss consequences of this relationship for AI: deep nets naturally weight invariant units over sensitive units, and this can be strengthened with training, perhaps contributing to generalization performance. Our results predict a signature relationship between invariance and dynamic range that can now be tested in future neurophysiological studies.",https://openreview.net/pdf/69682069866a0d8c50c3264554202dd66f8e84cf.pdf,stupidity,"['Dean A. Pospisil', 'Wyeth Bair']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1gKmmKULB,Contextual and neural representations of sequentially complex animal vocalizations,"['sequence learning', 'birdsong', 'auditory neuroscience', 'generative models', 'context']","Holistically exploring the perceptual and neural representations underlying animal communication has traditionally been very difficult because of the complexity of the underlying signal. We present here a novel set of techniques to project entire communicative repertoires into low dimensional spaces that can be systematically sampled from, exploring the relationship between perceptual representations, neural representations, and the latent representational spaces learned by machine learning algorithms. We showcase this method in one ongoing experiment studying sequential and temporal maintenance of context in songbird neural and perceptual representations of syllables. We further discuss how studying the neural mechanisms underlying the maintenance of the long-range information content present in birdsong can inform and be informed by machine sequence modeling.",https://openreview.net/pdf/1423d36ba7db786633fe1e71586b632d9565891f.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1eA7XtILS,Reinforcement Learning Models of Human Behavior: Reward Processing in Mental Disorders,"['neuroscience', 'reward processing', 'reinforcement learning', 'psychiatric disorders']","Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for a reinforcement learning problem, which extends the standard Q-learning approach to incorporate a two-stream framework of reward processing with biases biologically associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. For the AI community, the development of agents that react differently to different types of rewards can enable us to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Empirically, the proposed model outperforms Q-Learning and Double Q-Learning in artificial scenarios with certain reward distributions and real-world human decision making gambling tasks. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems. ",https://openreview.net/pdf/d08884e548bf5f3a93bf2bbbd96aacc7b523cf63.pdf,stupidity,"['Baihan Lin', 'Guillermo Cecchi', 'Djallel Bouneffouf', 'Jenna Reinen', 'Irina Rish']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyxTQ7K88S,Biologically-Inspired Spatial Neural Networks,"['deep learning', 'neuroscience', 'multi-task learning']","We introduce bio-inspired artificial neural networks consisting of neurons that are additionally characterized by spatial positions. To simulate properties of biological systems we add the costs penalizing long connections and the proximity of neurons in a two-dimensional space. Our experiments show that in the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior not only corresponds to the biological systems, but also allows for further insight into interpretability or continual learning.",https://openreview.net/pdf/3a906f5026965b9d0ea6c219d237878d6574618f.pdf,stupidity,"['Maciej Wołczyk', 'Jacek Tabor', 'Marek Śmieja', 'Szymon Maszke']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyxENQtL8H,Estimating encoding models of cortical auditory processing using naturalistic stimuli and transfer learning,"['neuroimaging', 'deep learning', 'transfer learning', 'audio', 'encoding models']","The purpose of an encoding model is to predict brain activity given a stimulus. In this contribution, we attempt at estimating a whole brain encoding model of auditory perception in a naturalistic stimulation setting. We analyze data from an open dataset, in which 16 subjects watched a short movie while their brain activity was being measured using functional MRI. We extracted feature vectors aligned with the timing of the audio from the movie, at different layers of a Deep Neural Network pretrained on the classification of auditory scenes. fMRI data was parcellated using hierarchical clustering on 500 parcels, and encoding models were estimated using a fully connected neural network with one hidden layer, trained to predict the signals for each parcel from the DNN features. Individual encoding models were successfully trained and predicted brain activity on unseen data, in parcels located in the superior temporal lobe, as well as dorsolateral prefrontal regions, which are usually considered as areas involved in auditory and language processing. Taken together, this contribution extends previous attempts on estimating encoding models, by showing the ability to model brain activity using a generic DNN (ie not specifically trained for this purpose) to extract auditory features, suggesting a degree of similarity between internal DNN representations and brain activity in naturalistic settings. ",https://openreview.net/pdf/567a27c5920b3fb483ef29aa98bcc2aea62ab57b.pdf,stupidity,"['Nicolas Farrugia', 'Victor Nepveu', 'Deycy Camila Arias Villamil']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Syx377Y8IH,Learning Non-Parametric Invariances from Data with Permanent Random Connectomes ,[],"One of the fundamental problems in supervised classification and in machine learning in general, is the modelling of non-parametric invariances that exist in data. Most prior art has focused on enforcing priors in the form of invariances to parametric nuisance transformations that are expected to be present in data. However, learning non-parametric invariances directly from data remains an important open problem. In this paper, we introduce a new architectural layer for convolutional networks which is capable of learning general invariances from data itself. This layer can learn invariance to non-parametric transformations and interestingly, motivates and incorporates permanent random connectomes there by being called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with random connections (not just weights) which are a small subset of the connections in a fully connected convolution layer. Importantly, these connections in PRC-NPTNs once initialized remain permanent throughout training and testing.  Random connectomes makes these architectures loosely more biologically plausible than many other mainstream network architectures which require highly ordered structures. We motivate randomly initialized connections as a simple method to learn invariance from data itself while invoking invariance towards multiple nuisance transformations simultaneously. We find that these randomly initialized permanent connections have positive effects on generalization, outperform much larger ConvNet baselines and the recently proposed Non-Parametric Transformation Network (NPTN) on benchmarks that enforce learning invariances from the data itself.",https://openreview.net/pdf/19fac9518beb7ccac1657ce4336c82e4d56aac10.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Syl0NmtLIr,Modelling Working Memory using Deep Recurrent Reinforcement Learning,"['deep learning', 'working memory', 'recurrent neural networks', 'reinforcement learning', 'brain modelling']","In cognitive systems, the role of a working memory is crucial for visual reasoning and decision making. Tremendous progress has been made in understanding the mechanisms of the human/animal working memory, as well as in formulating different frameworks of artificial neural networks.  In the case of humans, the visual working memory (VWM) task is a standard one in which the subjects are presented with a sequence of images, each of which needs to be identified as to whether it was already seen or not. 

Our work is a study of multiple ways to learn a working memory model using recurrent neural networks that learn to remember input images across timesteps. We train these neural networks to solve the working memory task by training them with a sequence of images in supervised and reinforcement learning settings. The supervised setting uses image sequences with their corresponding labels. The reinforcement learning setting is inspired by the popular view in neuroscience that the working memory in the prefrontal cortex is modulated by a dopaminergic mechanism. We consider the VWM task as an environment that rewards the agent when it remembers past information and penalizes it for forgetting. 
 
We quantitatively estimate the performance of these models on sequences of images from a standard image dataset (CIFAR-100). Further, we evaluate their ability to remember and recall as they are increasingly trained over episodes. Based on our analysis, we establish that a gated recurrent neural network model with long short-term memory units trained using reinforcement learning is powerful and more efficient in temporally consolidating the input spatial information. 

This work is an initial analysis as a part of our ultimate goal to use artificial neural networks to model the behavior and information processing of the working memory of the brain and to use brain imaging data captured from human subjects during the VWM cognitive task to understand various memory mechanisms of the brain. 
",https://openreview.net/pdf/03cdc8471aaf39a0a8f684b1bba0016f3849668f.pdf,stupidity,"['Pravish Sainath', 'Pierre Bellec', 'Guillaume Lajoie']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyeumQYUUH,"Predictive Coding, Variational Autoencoders, and Biological Connections","['predictive coding', 'variational autoencoders', 'probabilistic models', 'variational inference']","Predictive coding, within theoretical neuroscience, and variational autoencoders, within machine learning, both involve latent Gaussian models and variational inference. While these areas share a common origin, they have evolved largely independently. We outline connections and contrasts between these areas, using their relationships to identify new parallels between machine learning and neuroscience. We then discuss specific frontiers at this intersection: backpropagation, normalizing flows, and attention, with mutual benefits for both fields.",https://openreview.net/pdf/bbd45ab171a9841a5ca613af3afbb2699ff28659.pdf,stupidity,['Joseph Marino'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyerXXt8IS,Insect Cyborgs: Bio-mimetic Feature Generators Improve ML Accuracy  on Limited Data,"['feature selection', 'bio-mimesis', 'neural networks', 'insect olfaction', 'sparsity']","We seek to auto-generate stronger  input features  for ML methods faced with limited training data.
Biological neural nets (BNNs) excel at fast learning, implying that they extract highly informative features. In particular, the insect olfactory network  learns new odors very rapidly, by means of three key elements: A competitive inhibition layer; randomized, sparse connectivity into a high-dimensional sparse plastic layer; and Hebbian updates of synaptic weights. In this work we deploy MothNet, a computational model of the moth olfactory network, as an automatic feature generator. Attached as a front-end pre-processor, MothNet's readout neurons provide new features, derived from the original features, for use by standard ML classifiers. These ``insect cyborgs'' (part BNN and part ML method) have significantly better performance than baseline ML methods alone on vectorized MNIST and Omniglot data sets, reducing test set error averages 20% to 55%. The MothNet feature generator also substantially out-performs other feature generating methods including PCA, PLS, and NNs. These results highlight the potential value of BNN-inspired feature generators in the ML context.",https://openreview.net/pdf/e426b57255edb07cb1fed1b33f42bc578d15ce07.pdf,stupidity,"['Charles B. Delahunt', 'J. Nathan Kutz']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SyeWE7tU8H,Flexible degrees of connectivity under synaptic weight constraints,[],"Biological neural networks face homeostatic and resource constraints that restrict the allowed configurations of connection weights. If a constraint is tight it defines a very small solution space, and the size of these constraint spaces determines their potential overlap with the solutions for computational tasks. We study the geometry of the solution spaces for constraints on neurons' total synaptic weight and on individual synaptic weights, characterizing the connection degrees (numbers of partners) that maximize the size of these solution spaces. We then hypothesize that the size of constraints' solution spaces could serve as a cost function governing neural circuit development. We develop analytical approximations and bounds for the model evidence of the maximum entropy degree distributions under these cost functions. We test these on a published electron microscopic connectome of an associative learning center in the fly brain, finding evidence for a developmental progression in circuit structure.",https://openreview.net/pdf/d9cf1fe9dde790c818c70d848b753de9b9e42c85.pdf,stupidity,"['Gabriel Koch Ocker', 'Michael A. Buice']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SkxJ4QKIIS,Eligibility traces provide a data-inspired alternative to backpropagation through time,"['neuroscience', 'plausible learning rules', 'spiking neurons', 'BPTT', 'recurrent neural networks', 'LSTM', 'RNN', 'computational neuroscience', 'backpropagation through time', 'online learning', 'real-time recurrent learning', 'RTRL', 'eligibility traces']","Learning in recurrent neural networks (RNNs) is most often implemented by gradient descent using backpropagation through time (BPTT), but BPTT does not model accurately how the brain learns. Instead, many experimental results on synaptic plasticity can be summarized as three-factor learning rules involving eligibility traces of the local neural activity and a third factor. We present here eligibility propagation (e-prop), a new factorization of the loss gradients in RNNs that fits the framework of three factor learning rules when derived for biophysical spiking neuron models. When tested on the TIMIT speech recognition benchmark, it is competitive with BPTT both for training artificial LSTM networks and spiking RNNs. Further analysis suggests that the diversity of learning signals and the consideration of slow internal neural dynamics are decisive to the learning efficiency of e-prop.",https://openreview.net/pdf/58cd9e53436fcad3c17d6b8f932eee660c602792.pdf,stupidity,"['Guillaume Bellec', 'Franz Scherr', 'Elias Hajek', 'Darjan Salaj', 'Anand Subramoney', 'Robert Legenstein', 'Wolfgang Maass']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SklZVQtLLr,Disentangling the roles of dimensionality and cell classes in neural computations,"['RNN', 'reverse-engineering', 'mean-field theory', 'dimensionality', 'cell classes']","The description of neural computations in the field of neuroscience relies on two competing views: (i) a classical single-cell view that relates the activity of individual neurons to sensory or behavioural variables, and focuses on how different cell classes map onto computations; (ii) a more recent population view that instead characterises computations in terms of collective neural trajectories, and focuses on the dimensionality of these trajectories as animals perform tasks. How the two key concepts of cell classes and low-dimensional trajectories interact to shape neural computations is however currently not understood. Here we address this question by combining machine-learning tools for training RNNs with reverse-engineering and theoretical analyses of network dynamics. We introduce a novel class of theoretically tractable recurrent networks: low-rank, mixture of Gaussian RNNs. In these networks, the rank of the connectivity controls the dimensionality of the dynamics, while the number of components in the Gaussian mixture corresponds to the number of cell classes. Using back-propagation, we determine the minimum rank and number of cell classes needed to implement neuroscience tasks of increasing complexity. We then exploit mean-field theory to reverse-engineer the obtained solutions and identify the respective roles of dimensionality and cell classes. We show that the rank determines the phase-space available for dynamics that implement input-output mappings, while having multiple cell classes allows networks to flexibly switch between different types of dynamics in the available phase-space. Our results have implications for the analysis of neuroscience experiments and the development of explainable AI.",https://openreview.net/pdf/f9e575d223b3118b91d67317579bf66423ceb187.pdf,stupidity,"['Alexis M Dubreuil', 'Adrian Valente', 'Francesca Mastrogiuseppe', 'Srdjan Ostojic']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SkegNmFUIS,Significance of feedforward architectural differences between the ventral visual stream and DenseNet,"['vision', 'primate', 'deep learning']","There are many differences between convolutional networks and the ventral visual streams of primates. For example, standard convolutional networks lack recurrent and lateral connections, cell dynamics, etc. However, their feedforward architectures are somewhat similar to the ventral stream, and warrant a more detailed comparison. A recent study found that the feedforward architecture of the visual cortex could be closely approximated as a convolutional network, but the resulting architecture differed from widely used deep networks in several ways. The same study also found, somewhat surprisingly, that training the ventral stream of this network for object recognition resulted in poor performance. This paper examines the performance of this network in more detail. In particular, I made a number of changes to the ventral-stream-based architecture, to make it more like a DenseNet, and tested performance at each step. I chose DenseNet because it has a high BrainScore, and because it has some cortex-like architectural features such as large in-degrees and long skip connections. Most of the changes (which made the cortex-like network more like DenseNet) improved performance. Further work is needed to better understand these results. One possibility is that details of the ventral-stream architecture may be ill-suited to feedforward computation, simple processing units, and/or backpropagation, which could suggest differences between the way high-performance deep networks and the brain approach core object recognition.",https://openreview.net/pdf/4167a266df208ac76e87387554daad62e90f64e0.pdf,stupidity,['Bryan Tripp'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJxoX7K8LS,Coordinate-VAE: Unsupervised clustering and de-noising of peripheral nervous system data,"['Machine Learning', 'Peripheral Nervous System', 'Convolutional Neural Networks', 'Auto-encoder', 'Signal Processing']","The peripheral nervous system represents the input/output system for the brain. Cuff electrodes implanted on the peripheral nervous system allow observation and control over this system, however, the data produced by these electrodes have a low signal-to-noise ratio and a complex signal content. In this paper, we consider the analysis of neural data recorded from the vagus nerve in animal models, and develop an unsupervised learner based on convolutional neural networks that is able to simultaneously de-noise and cluster regions of the data by signal content.",https://openreview.net/pdf/27f296539e8906c55b5027030cb624f2495d4e7d.pdf,stupidity,"['Thomas J Hardcastle', 'Susannah Lee', 'Lorenz Wernisch', 'Pascal Fortier-Poisson', 'Sudha Shunmugam', 'Kalon Hewage', 'Tris Edwards', 'Oliver Armitage', 'Emil Hewage']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJl24XtLIB,Data-Driven Discovery of Functional Cell Types that Improve Models of Neural Activity,"['cell types', 'GLM', 'computational neuroscience', 'neural models']","Computational neuroscience aims to fit reliable models of in vivo neural activity and interpret them as abstract computations. Recent work has shown that functional diversity of neurons may be limited to that of relatively few cell types; other work has shown that incorporating constraints into artificial neural networks (ANNs) can improve their ability to mimic neural data. Here we develop an algorithm that takes as input recordings of neural activity and returns clusters of neurons by cell type and models of neural activity constrained by these clusters. The resulting models are both more predictive and more interpretable, revealing the contributions of functional cell types to neural computation and ultimately informing the design of future ANNs.",https://openreview.net/pdf/f809fb1da7dea6792c092db2c89ffe0c43858ed9.pdf,stupidity,"['Daniel Zdeblick', 'Eric Shea-Brown', 'Daniela Witten', 'Michael Buice']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJgHEQYLUS,Are skip connections necessary for biologically plausible learning rules?,"['Credit Assignment', 'Biologically plausible learning rule', 'skip connections']","Recognizing that backpropagation has been the workhorse of deep learning, it is time to explore other alternative learning methods. Several biologically motivated learning rules have been introduced, such as random feedback alignment and difference target propagation. However, none of these methods have produced competitive performance against backpropagation.  In this paper, we show that biologically motivated learning rules with skip connections between intermediate layers can perform as well as backpropagation on the MNIST dataset and is robust to various sets of hyper-parameters.",https://openreview.net/pdf/559fd3e1581992dfc089dc9f185cce69fe9496ac.pdf,stupidity,"['Daniel Jiwoong Im', 'Rutuja Patil', 'Kristin Branson']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SJewmXF88r,The Virtual Patch Clamp: Imputing C. elegans Membrane Potentials from Calcium Imaging,"['connectomics', 'optimisation', 'state-space estimation', 'simulation', 'c. elegans']","We develop a stochastic whole-brain and body simulator of the nematode roundworm Caenorhabditis elegans (C. elegans) and show that it is sufficiently regularizing to allow imputation of latent membrane potentials from partial calcium fluorescence imaging observations. This is the first attempt we know of to ``complete the circle,'' where an anatomically grounded whole-connectome simulator is used to impute a time-varying ``brain'' state at single-cell fidelity from covariates that are measurable in practice.  Using state of the art Bayesian machine learning methods to condition on readily obtainable data, our method paves the way for neuroscientists to recover interpretable connectome-wide state representations, automatically estimate physiologically relevant parameter values from data, and perform simulations investigating intelligent lifeforms in silico.",https://openreview.net/pdf/a55403148f9bdf91314d347a1f087859d4a4f2b8.pdf,stupidity,"['Andrew Warrington', 'Arthur Spencer', 'Frank Wood']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1zyEXYI8B,Towards learning principles of the brain and spiking neural networks,"['spiking neural networks', 'Spike-time dependent plasticity', 'network simulations']","The brain, the only system with general intelligence, is a network of spiking neurons (i.e., spiking neural networks, SNNs), and several neuromorphic chips have been developed to implement SNNs to build power-efficient learning systems. Naturally, both neuroscience and machine learning (ML) scientists are attracted to SNNs’ operating principles. Based on biologically plausible network simulations, we propose that spatially nonspecific top-down inputs, projected into lower-order areas from high-order areas, can enhance the brain’s learning process. Our study raises the possibility that training SNNs need novel mechanisms that do not exist in conventional artificial neural networks (ANNs) including deep neural networks (DNNs).    ",https://openreview.net/pdf/503bac69556c01c6da3b715c28ff69f886cab60b.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1xxRoLKLH,Adversarial Training of Neural Encoding Models on Population Spike Trains,"['neural encoding models', 'neural variability', 'GANs', 'visual system', 'conditional GANs']","Neural population responses to sensory stimuli can exhibit both nonlinear stimulus- dependence and richly structured shared variability. Here, we show how adversarial training can be used to optimize neural encoding models to capture both the deterministic and stochastic components of neural population data. To account for the discrete nature of neural spike trains, we use the REBAR method to estimate unbiased gradients for adversarial optimization of neural encoding models. We illustrate our approach on population recordings from primary visual cortex. We show that adding latent noise-sources to a convolutional neural network yields a model which captures both the stimulus-dependence and noise correlations of the population activity.",https://openreview.net/pdf/d0082d1d4094de2cc747ec7692c2713d26da4fe0.pdf,stupidity,"['Poornima Ramesh', 'Mohamad Atayi', 'Jakob H Macke']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1leV7t8IB,Unsupervised Discovery of Dynamic Neural Circuits,"['dynamic neural relational inference', 'variational autoencoder', 'cortical processing', 'neural dynamics', 'brain computation']","What can we learn about the functional organization of cortical microcircuits from large-scale recordings of neural activity?  To obtain an explicit and interpretable model of time-dependent functional connections between neurons and to establish the dynamics of the cortical information flow, we develop 'dynamic neural relational inference' (dNRI). We study  both synthetic and real-world neural spiking data and demonstrate that the developed method is able to uncover the dynamic relations between neurons more reliably than existing baselines.",https://openreview.net/pdf/2cb5581fdac814497ae20b7d64d42d0e533ca698.pdf,stupidity,"['Colin Graber', 'Ryan Loh', 'Yurii Vlasov', 'Alexander Schwing']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1gc4XF8Lr,Does the neuronal noise in cortex help generalization?,"['noise', 'trial-to-trial variability', 'subspace', 'generalization', 'dropout']","Neural activity is highly variable in response to repeated stimuli. We used an open dataset, the Allen Brain Observatory, to quantify the distribution of responses to repeated natural movie presentations. A large fraction of responses are best fit by log-normal distributions or Gaussian mixtures with two components. These distributions are similar to those from units in deep neural networks with dropout. Using a separate set of electrophysiological recordings, we constructed a population coupling model as a control for state-dependent activity fluctuations and found that the model residuals also show non-Gaussian distributions. We then analyzed responses across trials from multiple sections of different movie clips and observed that the noise in cortex aligns better with in-clip versus out-of-clip stimulus variations. We argue that noise is useful for generalization when it moves along representations of different exemplars in-class, similar to the structure of cortical noise.",https://openreview.net/pdf/530499ff8cf42f3c67535d1520d47a1680b24b5c.pdf,stupidity,"['Brian Hu', 'Jiaqi Shang', 'Ramakrishnan Iyer', 'Josh Siegle', 'Stefan Mihalas']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1g_N7FIUS,Neocortical plasticity: an unsupervised cake but no free lunch,"['neocortex', 'local learning', 'dendrites', 'adversarial examples', 'generalisation']","The fields of artificial intelligence and neuroscience have a long history of fertile bi-directional interactions. On the one hand, important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence, the mammalian neocortex in particular. On the other, important inspiration for models and theories of the brain have emerged from artificial intelligence research. A central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm of deep networks. Matching the data efficiency, transfer and generalisation properties of neocortical learning remains an area of active research in the field of deep learning. Recent advances in our understanding of neuronal, synaptic and dendritic physiology of the neocortex suggest new approaches for unsupervised representation learning, perhaps through a new class of objective functions, which could act alongside or in lieu of back-propagation. Such local learning rules have implicit rather than explicit objectives with respect to the training data, facilitating domain adaptation and generalisation.  Incorporating them into deep networks for representation learning could better leverage unlabelled datasets to offer significant improvements in data efficiency of downstream supervised readout learning, and reduce susceptibility to adversarial perturbations, at the cost of a more restricted domain of applicability.
",https://openreview.net/pdf/e014d06826874b295a1c30bd9b20630118d16fad.pdf,stupidity,"['Eilif B. Muller', 'Philippe Beaudoin']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hylu4mYIIS,Automated Animal Training and Iterative Inference of Latent Learning Policy,"['learning', 'neuroscience', 'behavior', 'automated training', 'latent learning', 'visual discrimination', 'automated analysis', 'reinforcement learning', 'behavior analysis', 'policy inference', 'behavior prediction']","Progress in understanding how individual animals learn requires high-throughput standardized methods for behavioral training and ways of adapting training. During the course of training with hundreds or thousands of trials, an animal may change its underlying strategy abruptly, and capturing these changes requires real-time inference of the animal’s latent decision-making strategy. To address this challenge, we have developed an integrated platform for automated animal training, and an iterative decision-inference model that is able to infer the momentary decision-making policy, and predict the animal’s choice on each trial with an accuracy of ~80\%, even when the animal is performing poorly. We also combined decision predictions at single-trial resolution with automated pose estimation to assess movement trajectories. Analysis of these features revealed categories of movement trajectories that associate with decision confidence.",https://openreview.net/pdf/14670431577ffa61a1feabff1650c73466d0a676.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hyl_XXYLIB,Continual Learning via Neural Pruning,"['life-long learning', 'catastrophic forgetting']","Inspired by the modularity and the life-cycle of biological neurons,we introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on the pruning of neurons of low activity. In this method, an L1 regulator is used to promote the presence of neurons of zero or low activity whose connections to previously active neurons is permanently severed at the end of training. Subsequent tasks are trained using these pruned neurons after reinitialization and cause zero deterioration to the performance of previous tasks. We show empirically that this biologically inspired method leads to state of the art results beating or matching current methods of higher computational complexity.",https://openreview.net/pdf/aba957bbea9f76cf45c6a917123ccdb45a713101.pdf,stupidity,"['Siavash Golkar', 'Micheal Kagan', 'Kyunghyun Cho']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Hye5NQYU8r,Do deep neural networks possess concept space grid cells?,"['concept space', 'cognitive map', 'place cells', 'grid cells', 'memory retrieval']","Place and grid-cells are known to aid navigation in animals and humans. Together with concept cells, they allow humans to form an internal representation of the external world, namely the concept space. We investigate the presence of such a space in deep neural networks by plotting the activation profile of its hidden layer neurons. Although place cell and concept-cell like properties are found, grid-cell like firing patterns are absent thereby indicating a lack of path integration or feature transformation functionality in trained networks. Overall, we present a plausible inadequacy in current deep learning practices that restrict deep networks from performing analogical reasoning and memory retrieval tasks.",https://openreview.net/pdf/c1985e18e0560577503f0a99a306f3b5ccadc5bb.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HklfNQFL8H,Learning to Learn with Feedback and Local Plasticity,"['biologically plausible learning', 'meta learning']","Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. It can be shown mathematically that this approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based meta-learning algorithm for continual learning on regression and classification benchmarks. This approach represents a step toward biologically plausible learning mechanisms that can not only match gradient descent-based learning, but also overcome its limitations.",https://openreview.net/pdf/4b1395dc5879a1a37a70cc0d6c35053491065ea5.pdf,stupidity,['Jack Lindsey'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HkgVE7YUIH,Efficient rescue of damaged neural networks,"['neural networks', 'resilience', 'dynamical systems', 'attractors']","Neural networks in the brain and in neuromorphic chips confer systems with the ability to perform multiple cognitive tasks. However, both kinds of networks experience a wide range of physical perturbations, ranging from damage to edges of the network to complete node deletions, that ultimately could lead to network failure. A critical question is to understand how the computational properties of neural networks change in response to node-damage and whether there exist strategies to repair these networks in order to compensate for performance degradation. Here, we study the damage-response characteristics of two classes of neural networks, namely multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) trained to classify images from MNIST and CIFAR-10 datasets respectively. We also propose a new framework to discover efficient repair strategies to rescue damaged neural networks. The framework involves defining damage and repair operators for dynamically traversing the neural networks loss landscape, with the goal of mapping its salient geometric features. Using this strategy, we discover features that resemble path-connected attractor sets in the loss landscape. We also identify that a dynamic recovery scheme, where networks are constantly damaged and repaired, produces a group of networks resilient to damage as it can be quickly rescued. Broadly, our work shows that we can design fault-tolerant networks by applying on-line retraining consistently during damage for real-time applications in biology and machine learning.",https://openreview.net/pdf/0d3c5bf8874475edb2e7afac612bf499ec3b70f2.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HkgSXQtIIB,On the Adversarial Robustness of Neural Networks without Weight Transport,"['Neural networks without weight transport', 'gradient-based adversarial attacks']","Neural networks trained with backpropagation, the standard algorithm of deep learning which uses weight transport, are easily fooled by existing gradient-based adversarial attacks. This class of attacks are based on certain small perturbations of the inputs to make networks misclassify them. We show that less biologically implausible deep neural networks trained with feedback alignment, which do not use weight transport, can be harder to fool, providing actual robustness. Tested on MNIST, deep neural networks trained without weight transport (1) have an adversarial accuracy of 98% compared to 0.03% for neural networks trained with backpropagation and (2) generate non-transferable adversarial examples. However, this gap decreases on CIFAR-10 but is still significant particularly for small perturbation magnitude less than 1 ⁄ 2.",https://openreview.net/pdf/d52ad80b6b4c1d540986b94510c77163134d5235.pdf,stupidity,['Mohamed Akrout'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJlVEQt8Lr,Revisit Recurrent Attention Model from an Active Sampling Perspective,[],"We revisit the Recurrent Attention Model (RAM, Mnih et al. (2014)), a recurrent neural network for visual attention, from an active information sampling perspective. 

We borrow ideas from neuroscience research on the role of active information sampling in the context of visual attention and gaze (Gottlieb, 2018), where the author suggested three types of motives for active information sampling strategies. We find the original RAM model only implements one of them.

We identify three key weakness of the original RAM and provide a simple solution by adding two extra terms on the objective function. The modified RAM 1) achieves faster convergence, 2) allows dynamic decision making per sample without loss of accuracy, and 3) generalizes much better on longer sequence of glimpses which is not trained for, compared with the original RAM. 
",https://openreview.net/pdf/f8da9fb722ff669f02f69e208e234890ddc343e4.pdf,stupidity,['Jialin Lu'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJlKNmFIUB,Augmenting Supervised Learning by Meta-learning Unsupervised Local Rules,"['Hebbian learning', 'deep learning optimization', 'metalearning', 'learning to learn']","The brain performs unsupervised learning and (perhaps) simultaneous supervised learning. This raises the question as to whether a hybrid of supervised and unsupervised methods will produce better learning. Inspired by the rich space of Hebbian learning rules, we set out to directly learn the unsupervised learning rule on local information that best augments a supervised signal. We present the Hebbian-augmented training algorithm (HAT) for combining gradient-based learning with an unsupervised rule on pre-synpatic activity, post-synaptic activities, and current weights. We test HAT's effect on a simple problem (Fashion-MNIST) and find consistently higher performance than supervised learning alone. This finding provides empirical evidence that unsupervised learning on synaptic activities provides a strong signal that can be used to augment gradient-based methods.
    
    We further find that the meta-learned update rule is a time-varying function; thus, it is difficult to pinpoint an interpretable Hebbian update rule that aids in training.  We do find that the meta-learner eventually degenerates into a non-Hebbian rule that preserves important weights so as not to disturb the learner's convergence.",https://openreview.net/pdf/f7cd10118acd21c3d7a68ad5f093e8f6ce3a6059.pdf,stupidity,"['Jeffrey Siedar Cheng', 'Ari Benjamin', 'Benjamin Lansdell', 'Konrad Paul Kording']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJgiVXY88r,Learning to predict visual brain activity by predicting future sensory states,"['predictive coding', 'representational similarity analysis', 'visual brain', 'fmri']","Deep predictive coding networks are neuroscience-inspired unsupervised learning models that learn to predict future sensory states. We build upon the PredNet implementation by Lotter, Kreiman, and Cox (2016) to investigate if predictive coding representations are useful to predict brain activity in the visual cortex. We use representational similarity analysis (RSA) to compare PredNet representations to functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) data from the Algonauts Project (Cichy et al., 2019). In contrast to previous findings in the literature (Khaligh-Razavi & Kriegeskorte, 2014), we report empirical data suggesting that unsupervised models trained to predict frames of videos without further fine-tuning may outperform supervised image classification baselines in terms of correlation to spatial (fMRI) and temporal (MEG) data.",https://openreview.net/pdf/bf5377bf7c8e4217a6166dc075e13836afe71f3d.pdf,stupidity,['Marcio Fonseca'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJghQ7YU8H,Checking Functional Modularity in DNN By Biclustering Task-specific Hidden Neurons,"['DNN', 'modularity']","While real brain networks exhibit functional modularity, we investigate whether functional mod- ularity also exists in Deep Neural Networks (DNN) trained through back-propagation. Under the hypothesis that DNN are also organized in task-specific modules, in this paper we seek to dissect a hidden layer into disjoint groups of task-specific hidden neurons with the help of relatively well- studied neuron attribution methods. By saying task-specific, we mean the hidden neurons in the same group are functionally related for predicting a set of similar data samples, i.e. samples with similar feature patterns.
We argue that such groups of neurons which we call Functional Modules can serve as the basic functional unit in DNN. We propose a preliminary method to identify Functional Modules via bi- clustering attribution scores of hidden neurons.
We find that first, unsurprisingly, the functional neurons are highly sparse, i.e., only a small sub- set of neurons are important for predicting a small subset of data samples and, while we do not use any label supervision, samples corresponding to the same group (bicluster) show surprisingly coherent feature patterns. We also show that these Functional Modules perform a critical role in discriminating data samples through ablation experiment. ",https://openreview.net/pdf/87ad96fc01ff4f2cdc79e49ddf7608c8e576286d.pdf,stupidity,"['Jialin Lu', 'Martin Ester']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJgPEXtIUS,Evaluating biological plausibility of learning algorithms the lazy way,"['Machine learning', 'back propagation through time', 'biological plausibility', 'online learning']",To which extent can successful machine learning inform our understanding of biological learning? One popular avenue of inquiry in recent years has been to directly map such algorithms into a realistic circuit implementation. Here we focus on learning in recurrent networks and investigate a range of learning algorithms. Our approach decomposes them into their computational building blocks and discusses their abstract potential as biological operations. This alternative strategy provides a “lazy” but principled way of evaluating ML ideas in terms of their biological plausibility,https://openreview.net/pdf/b7ef295ad76b036ef6f615fba1a0100446d4a5c4.pdf,stupidity,"['Owen Marschall', 'Kyunghyun Cho', 'Cristina Savin']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HJenmmF8Ir,Functional Annotation of Human Cognitive States using Graph Convolution Networks,"['fMRI', 'functional connectivity', 'brain decoding', 'graph convolutional network', 'deep learning']","A key goal in neuroscience is to understand brain mechanisms of cognitive functions. An emerging approach is to study “brain states” dynamics using functional magnetic resonance imaging (fMRI). So far in the literature, brain states have typically been studied using 30 seconds of fMRI data or more, and it is unclear to which extent brain states can be reliably identified from very short time series. In this project, we applied graph convolutional networks (GCN) to decode brain activity over short time windows in a task fMRI dataset, i.e. associate a given window of fMRI time series with the task used. Starting with a populational brain graph with nodes defined by a parcellation of cerebral cortex and the adjacent matrix extracted from functional connectome, GCN takes a short series of fMRI volumes as input, generates high-level domain-specific graph representations, and then predicts the corresponding cognitive state. We investigated the performance of this GCN ""cognitive state annotation"" in the Human Connectome Project (HCP) database, which features 21 different experimental conditions spanning seven major cognitive domains, and high temporal resolution task fMRI data. Using a 10-second window, the 21 cognitive states were identified with an excellent average test accuracy of 89% (chance level 4.8%). As the HCP task battery was designed to selectively activate a wide range of specialized functional networks, we anticipate the GCN annotation to be applicable as a base model for other transfer learning applications, for instance, adapting to new task domains.",https://openreview.net/pdf/5e585836f2a4e5833e76d1a90535c31a380fc5c0.pdf,stupidity,"['Yu Zhang', 'Pierre Bellec']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1xI7XYULr,Cellular neuromodulation in artificial networks,"['neuromodulation', 'deep learning', 'reinforcement learning']","Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such adaptation property strongly relies on cellular neuromodulation, the biological mechanism that dynamically controls neuron intrinsic properties and response to external stimuli in a context dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems.",https://openreview.net/pdf/290d2f318fb850252228edd433c06bd35415431e.pdf,stupidity,"['Vecoven Nicolas', 'Ernst Damien', 'Wehenkel Antoine', 'Drion Guillaume']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ByxfNXF8Ir,Learning to solve the credit assignment problem,"['biologically plausible deep learning', 'feedback alignment', 'REINFORCE', 'node perturbation']","Backpropagation is driving today's artificial neural networks. However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach, in which each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We show that our approach learns to approximate the gradient, and can match the performance of gradient-based learning on fully connected and convolutional networks. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.",https://openreview.net/pdf/ea8503a9157a8fc388477eb4bdd4584133abcad6.pdf,stupidity,"['Benjamin James Lansdell', 'Prashanth Prakash', 'Konrad Paul Kording']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BylmV7tI8S,Recurrent neural networks learn robust representations by dynamically balancing compression and expansion,"['Recurrent Neural Network', 'Temporal Learning', 'Chaotic Dynamics', 'Dimensionality', 'Working Memory']","Recordings of neural circuits in the brain reveal extraordinary dynamical richness and high variability. At the same time, dimensionality reduction techniques generally uncover low-dimensional structures underlying these dynamics. What determines the dimensionality of activity in neural circuits? What is the functional role of dimensionality in behavior and task learning? In this work we address these questions using recurrent neural network (RNN) models. We find that, depending on the dynamics of the initial network, RNNs learn to increase and reduce dimensionality in a way that matches task demands. These findings shed light on fundamental dynamical mechanisms by which neural networks solve tasks with robust representations that generalize to new cases.",https://openreview.net/pdf/ff04faeb440fbdb7c67f4ed9ad95e2cf06290ced.pdf,stupidity,"['Matthew Farrell', 'Stefano Recanatesi', 'Guillaume Lajoie', 'Eric Shea-Brown']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BylUXXFI8S,Evolving the Olfactory System,"['evolution', 'perception', 'olfaction', 'connectivity']","Flies and mice are species separated by 600 million years of evolution, yet have evolved olfactory systems that share many similarities in their anatomic and functional organization. What functions do these shared anatomical and functional features serve, and are they optimal for odor sensing? In this study, we address the optimality of evolutionary design in olfactory circuits by studying artificial neural networks trained to sense odors. We found that artificial neural networks quantitatively recapitulate structures inherent in the olfactory system, including the formation of glomeruli onto a compression layer and sparse and random connectivity onto an expansion layer. Finally, we offer theoretical justifications for each result. Our work offers a framework to explain the evolutionary convergence of olfactory circuits, and gives insight and logic into the anatomic and functional structure of the olfactory system.",https://openreview.net/pdf/028c1846f01aaba2c4064b8da8d304167e508389.pdf,stupidity,"['Robert Guangyu Yang', 'Peter Yiliu Wang', 'Yi Sun', 'Ashok Litwin-Kumar', 'Richard Axel', 'LF Abbott']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Byg1E7KIIr,Spike Sorting using the Neural Clustering Process,"['spike sorting', 'neural clustering process', 'bayesian clustering', 'amortization']","We present a novel approach to spike sorting for high-density multielectrode probes using the Neural Clustering Process (NCP), a recently introduced neural architecture that performs scalable amortized approximate Bayesian inference for efficient probabilistic clustering. To optimally encode spike waveforms for clustering, we extended NCP by adding a convolutional spike encoder, which is learned end-to-end with the NCP network. Trained purely on labeled synthetic spikes from a simple generative model, the NCP spike sorting model shows promising performance for clustering multi-channel spike waveforms. The model provides higher clustering quality than an alternative Bayesian algorithm, finds more spike templates with clear receptive fields on real data and recovers more ground truth neurons on hybrid test data compared to a recent spike sorting algorithm. Furthermore, NCP is able to handle the clustering uncertainty of ambiguous small spikes by GPU-parallelized posterior sampling. The source code is publicly available.",https://openreview.net/pdf/4b82faee592cab86045499b3c95ee6a34694559b.pdf,stupidity,"['Yueqi Wang', 'Ari Pakman', 'Catalin Mitelut', 'JinHyung Lee', 'Liam Paninski']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ByMLEXFIUS,Revealing computational mechanisms of retinal prediction via model reduction,[],"Recently, deep feedforward neural networks have achieved considerable success in modeling biological sensory processing, in terms of reproducing the input-output map of sensory neurons. However, such models raise profound questions about the very nature of explanation in neuroscience. Are we simply replacing one complex system (a biological circuit) with another (a deep network), without understanding either? Moreover, beyond neural representations, are the deep network's {\it computational mechanisms} for generating neural responses the same as those in the brain? Without a systematic approach to extracting and understanding computational mechanisms from deep neural network models, it can be difficult both to assess the degree of utility of deep learning approaches in neuroscience, and to extract experimentally testable hypotheses from deep networks. We develop such a systematic approach by combining dimensionality reduction and modern attribution methods for determining the relative importance of interneurons for specific visual computations. We apply this approach to deep network models of the retina, revealing a conceptual understanding of how the retina acts as a predictive feature extractor that signals deviations from expectations for diverse spatiotemporal stimuli. For each stimulus, our extracted computational mechanisms are consistent with prior scientific literature, and in one case yields a new mechanistic hypothesis. Thus overall, this work not only yields insights into the computational mechanisms underlying the striking predictive capabilities of the retina, but also places the framework of deep networks as neuroscientific models on firmer theoretical foundations, by providing a new roadmap to go beyond comparing neural representations to extracting and understand computational mechanisms.",https://openreview.net/pdf/5677e102e74d8908fb1a8d2b7e0a650fff23aa24.pdf,stupidity,"['Hidenori Tanaka', 'Aran Nayebi', 'Niru Maheswaranathan', 'Lane McIntosh', 'Stephen A. Baccus', 'Surya Ganguli']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BkxsVXtLUr,Learning a Convolutional Bilinear Sparse Code for Natural Videos,"['Unsupervised Learning', 'Spatio-Temporal Features', 'Sparse Coding', 'Equivariance', 'Capsules']","In contrast to the monolithic deep architectures used in deep learning today for computer vision, the visual cortex processes retinal images via two functionally distinct but interconnected networks: the ventral pathway for processing object-related information and the dorsal pathway for processing motion and transformations. Inspired by this cortical division of labor and properties of the magno- and parvocellular systems, we explore an unsupervised approach to feature learning that jointly learns object features and their transformations from natural videos. We propose a new convolutional bilinear sparse coding model that (1) allows independent feature transformations and (2) is capable of processing large images. Our learning procedure leverages smooth motion in natural videos. Our results show that our model can learn groups of features and their transformations directly from natural videos in a completely unsupervised manner. The learned ""dynamic filters"" exhibit certain equivariance properties, resemble cortical spatiotemporal filters, and capture the statistics of transitions between video frames. Our model can be viewed as one of the first approaches to demonstrate unsupervised learning of primary ""capsules"" (proposed by Hinton and colleagues for supervised learning) and has strong connections to the Lie group approach to visual perception.",https://openreview.net/pdf/46dac219ebe9800896ac2387a695f2e5129d1faa.pdf,stupidity,"['Dimitrios C. Gklezakos', 'Rajesh P. N. Rao']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Bkfw77FLIS,Pattern recognition of labeled concepts by a single spiking neuron model.,"['spiking neural networks', 'neual plasticity', 'pattern recognition', 'single neuron', 'classification']","Making an informed, correct and quick decision can be life-saving. It's crucial for animals during an escape behaviour or for autonomous cars during driving. The decision can be complex and may involve an assessment of the amount of threats present and the nature of each threat. Thus, we should expect early sensory processing to supply classification information fast and accurately, even before relying the information to higher brain areas or more complex system components downstream. Today, advanced convolution artificial neural networks can successfully solve such tasks and are commonly used to build complex decision making systems. However, in order to achieve excellent performance on these tasks they require increasingly complex, ""very deep"" model structure, which is costly in inference run-time, energy consumption and number of training samples, only trainable on cloud-computing clusters.
A single spiking neuron has been shown to be able to solve many of these required tasks for homogeneous Poisson input statistics, a commonly used model for spiking activity in the neocortex; when modeled as leaky integrate and fire with gradient decent learning algorithm it was shown to posses a wide variety of complex computational capabilities. Here we refine its learning algorithm. The refined gradient-based local learning rule allows for better and stable generalization. We take advantage of this improvement to solve a problem of multiple instance learning (MIL) with counting where labels are only available for collections of concepts. We use an MNIST task  to show that the neuron indeed exploits the improvements and performs on par with conventional ConvNet architecture with similar parameter space size and number of training epochs.",https://openreview.net/pdf/5a5a208013ca3fadc74d4039c52f9580aa1ec4a9.pdf,stupidity,"['Hannes Rapp', 'Martin Paul Nawrot', 'Merav Stern']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=BJg6EmYL8B,Deep Connectomics Networks: Neural Network Architectures Inspired by Neuronal Networks,"['Network Neuroscience', 'neurons', 'brain', 'visual cortex', 'Deep Learning', 'mouse visual cortex', 'C. Elegans']","The interplay between inter-neuronal network topology and cognition has been studied deeply by connectomics researchers and network scientists, which is crucial towards understanding the remarkable efficacy of biological neural networks. Curiously, the deep learning revolution that revived neural networks has not paid much attention to topological aspects. The architectures of deep neural networks (DNNs) do not resemble their biological counterparts in the topological sense. We bridge this gap by presenting initial results of Deep Connectomics Networks (DCNs) as DNNs with topologies inspired by real-world neuronal networks. We show high classification accuracy obtained by DCNs whose architecture was inspired by the biological neuronal networks of C. Elegans and the mouse visual cortex.",https://openreview.net/pdf/19838b8f7ee9341806c76b24f5edcd8af9f5754d.pdf,stupidity,"['Nicholas Roberts', 'Dian Ang Yap', 'Vinay Uday Prabhu']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1lj77F88B,Translating neural signals to text using a Brain-Computer Interface,"['Brain-Computer interface', 'Speech decoding', 'Neural signal processing']","Brain-Computer Interfaces (BCI) may help patients with faltering communication abilities due to neurodegenerative diseases produce text or speech by direct neural processing. However, their practical realization has proven difficult due to limitations in speed, accuracy, and generalizability of existing interfaces. To this end, we aim to create a BCI that decodes text directly from neural signals. We implement a framework that initially isolates frequency bands in the input signal encapsulating differential information regarding production of various phonemic classes. These bands form a feature set that feeds into an LSTM which discerns at each time point probability distributions across all phonemes uttered by a subject. Finally, a particle filtering algorithm temporally smooths these probabilities incorporating prior knowledge of the English language to output text corresponding to the decoded word. Further, in producing an output, we abstain from constraining the reconstructed word to be from a given bag-of-words, unlike previous studies. The empirical success of our proposed approach, offers promise for the employment of such an interface by patients in unfettered, naturalistic environments.",https://openreview.net/pdf/9801301a950b2cd298ffabfe7bb6c6127a522b92.pdf,stupidity,"['Janaki Sheth', 'Ariel Tankus', 'Michelle Tran', 'Nader Pouratian', 'Itzhak Fried', 'William Speier']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1lKNXYU8S,What’s in a functional brain parcellation?,"['brain atlases', 'functional units']","To communicate, to ground hypotheses, to analyse data, neuroscientists often refer to divisions of the brain. Here we consider atlases used to parcellate the brain when studying brain function. We discuss the meaning and the validity of these parcellations, from a conceptual point of view as well as by running various analytical tasks on popular functional brain parcellations.",https://openreview.net/pdf/a7ac0b4f9eb8e870a6a3c4eaba609308020dd409.pdf,stupidity,"['Gaël Varoquaux', 'Kamalakar Dadi', 'Arthur Mensch']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1g0QmtIIS,Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks,"['reservoir networks', 'recurrent neural networks', 'local rules', 'Hebbian rules', 'continuous attractors']","Reservoir computing is a powerful tool to explain how the brain learns temporal sequences, such as movements, but existing learning schemes are either biologically implausible or too inefficient to explain animal performance. We show that a network can learn complicated sequences with a reward-modulated Hebbian learning rule if the network of reservoir neurons is combined with a second network that serves as a dynamic working memory and provides a spatio-temporal backbone signal to the reservoir. In combination with the working memory, reward-modulated Hebbian learning of the readout neurons performs as well as FORCE learning, but with the advantage of a biologically plausible interpretation of both the learning rule and the learning paradigm.",https://openreview.net/pdf/3b48a3c3f3d6a46fcb3c035ad06725fad57941f4.pdf,stupidity,"['Roman Pogodin', 'Dane Corneil', 'Alexander Seeholzer', 'Joseph Heng', 'Wulfram Gerstner']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1em4mFL8H,Spiking Recurrent Networks as a Model to Probe Neuronal Timescales Specific to Working Memory,"['Working memory', 'recurrent neural networks', 'neuronal timescales']","Cortical neurons process and integrate information on multiple timescales. In addition, these timescales or temporal receptive fields display functional and hierarchical organization. For instance, areas important for working memory (WM), such as prefrontal cortex, utilize neurons with stable temporal receptive fields and long timescales to support reliable representations of stimuli. Despite of the recent advances in experimental techniques, the underlying mechanisms for the emergence of neuronal timescales long enough to support WM are unclear and challenging to investigate experimentally. Here, we demonstrate that spiking recurrent neural networks (RNNs) designed to perform a WM task reproduce previously observed experimental findings and that these models could be utilized in the future to study how neuronal timescales specific to WM emerge.",https://openreview.net/pdf/ef66ac4942695d5eb04960a2be81c79594fe43b3.pdf,stupidity,"['Robert Kim', 'Terrence J. Sejnowski']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1eh4mYIUB,Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity,"['biological plausibility', 'dopaminergic plasticity', 'allele frequency', 'neural net evolution']","Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal’s genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task through minibatches. The relative performance of the two alleles of each gene is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. NNE is also tested on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs (replacing backprop) inspired by the recent discovery of dopaminergic plasticity. ",https://openreview.net/pdf/8b51f18d77751e93590a3ad5391b6151e1da8a46.pdf,stupidity,"['Sruthi Gorantla', 'Anand Louis', 'Christos H. Papadimitriou', 'Santosh Vempala', 'Naganand Yadati']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1eWVQYULB,Inferring hierarchies of latent features in calcium imaging data,"['calcium imaging', 'LFADS', 'variational autoencoders', 'dynamics', 'recurrent neural networks']","A key problem in neuroscience and life sciences more generally is that the data generation process is often best thought of as a hierarchy of dynamic systems. One example of this is in-vivo calcium imaging data, where observed calcium transients are driven by a combination of electro-chemical kinetics where hypothesized trajectories around manifolds determining the frequency of these transients. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamic structure of reaching behaviour from spiking data modelled as a Poisson process. Here we extend this approach using a ladder method to infer the spiking events driving calcium transients along with the deeper latent dynamic system. We show strong performance of this approach on a benchmark synthetic dataset against a number of alternatives.",https://openreview.net/pdf/f239eaae246a4e1f1240eda247ebffc09102a194.pdf,stupidity,"['Luke Y. Prince', 'Blake A. Richards']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1eU47t8Ir,Reinforcement learning with a network of spiking agents,"['Reinforcement learning', 'multi-agent learning', 'spiking neurons']","Neuroscientific theory suggests that dopaminergic neurons broadcast global reward prediction errors to large areas of the brain influencing the synaptic plasticity of the neurons in those regions (Schultz et al.). We build on this theory to propose a multi-agent learning framework with spiking neurons in the generalized linear model (GLM) formulation as agents, to solve reinforcement learning (RL) tasks. We show that a network of GLM spiking agents connected in a hierarchical fashion, where each spiking agent modulates its firing policy based on local information and a global prediction error, can learn complex action representations to solve RL tasks. We further show how leveraging principles of modularity and population coding inspired from the brain can help reduce variance in the learning updates making it a viable optimization technique.",https://openreview.net/pdf/849829b6daf10306da35c1b140d10e71925b94f3.pdf,stupidity,"['Sneha Aenugu', 'Abhishek Sharma', 'Sasikiran Yelamarthy', 'Hananel Hazan', 'Philip.S.Thomas', 'Robert Kozma']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1Mo4XFL8H,Differentiating Granger Causal Influence and Stimulus-Related Information Flow,"['Information Flow', 'Granger Causality', 'Interpreting Network Activity', 'Connectivity']","Information flow is becoming an increasingly popular term in the context of understanding neural circuitry, both in neuroscience and in Artificial Neural Networks. Granger causality has long been the tool of choice in the neuroscience literature for identifying functional connectivity in the brain, i.e., pathways along which information flows. However, there has been relatively little work on providing a fundamental theory for information flow, and as part of that, understanding whether Granger causality captures the intuitive direction of information flow in a computational circuit. Recently, Venkatesh et al. [2019] proposed a theoretical framework for identifying stimulus-related information paths in a computational graph. They also provided a counterexample showing that the direction of greater Granger causal influence can be opposite to that of information flow [Venkatesh and Grover, 2015]. Here, we reexamine and expand on this counterexample. In particular, we find that Granger Causal influence can be statistically insignificant in the direction of information flow, while being significant in the opposite direction. By examining the mutual- (and conditional-mutual-) information that each signal shares with the stimulus, we are able to gain a more nuanced understanding of the actual information flows in this system.",https://openreview.net/pdf/c666387a41cce4971137787b7862250b9485cee3.pdf,stupidity,['Anonymous'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryxuz9SzDB,The Differentiable Curry,"['automatic differentiation', 'higher-order programs']","We revisit the automatic differentiation (AD) of programs that contain higher-order functions, in a statically typed setting. Our presentation builds on a recent formulation of AD based on categorical combinators, and shows how that formulation can be extended to higher-order functions via two different notions of differentiable currying and evaluation. We present these alternative implementations, and justify their correctness by means of showing that AD yields equivalent back-propagators for forward-equivalent programs, even in the presence of higher-order features. Higher-order functions complicate the definition of equivalence for back-propagators compared to simpler forms of program equivalence.",https://openreview.net/pdf/41d18c30ba3d9acf608d7125b9944eb94f98c017.pdf,stupidity,"['Dimitrios Vytiniotis', 'Dan Belov', 'Richard Wei', 'Gordon Plotkin', 'Martin Abadi']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=ryllpA21vB,Differentiation of High-Level Language Semantics,[],"Though analytic differentiation (AD) is a program transformation, AD tools have typically supported only very limited program representations, consisting of primitive mathematical operations and basic structured control flow. Zygote, an AD for the Julia language, instead operates on Julia code. This presents an interesting challenge for the AD implementor: the program representation now contains not just mathematical operations, but arbitrary control flow, user-defined functions, recursion, data structures, mutation, metaprogramming, foreign function calls, specialised hardware, and even concurrency and parallelism primitives. This paper explains how Zygote handles these high-level features safely and efficiently, making an unusually large set of Julia programs differentiable.",https://openreview.net/pdf/018e2fd4974f44f84121ac3cf12ef051dba54cd1.pdf,stupidity,['Michael Innes'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rkgzj5Za8H,PyMC4: Exploiting Coroutines for Implementing a Probabilistic Programming Framework,"['python', 'statistical-analysis', 'bayesian inference', 'mcmc', 'variational-inference', 'probabilistic-programming']","PyMC4 is an open-source probabilistic programming library whose goal is to give users access to cutting-edge algorithms in Bayesian statistical computing while being extensible enough to help researchers to implement novel algorithms. Like its predecessor PyMC3 and the C++ library Stan, the aim of PyMC4 is to provide users with a high-level API to specify probabilistic models. Our focus is on Bayesian models where inference can be performed using (dynamic) Hamiltonian Monte Carlo and variational inference, both of which require automatic differentiation of user-defined joint density functions.",https://openreview.net/pdf/bb7c3ec4b869b1afd291fca80f748e12a01faa72.pdf,stupidity,"['Max Kochurov', 'Colin Carroll', 'Thomas Wiecki', 'Junpeng Lao']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJxd7vsWPS,Dex: array programming with typed indices,"['Array programming', 'functional programming']","Array programming is harder than it should be. Major pain points are managing bulk operations on high-rank arrays, and the associated shape and indexing errors. We describe Dex, a functional array processing language in the Haskell/ML family. Dex introduces a lightweight looping construct and a type system that captures common patterns of array shapes. We hope the language ideas we present here can influence the design of existing array programming systems.
",https://openreview.net/pdf/437fc1ea0ef9746003fe78985f1394e66608f4bf.pdf,stupidity,"['Dougal Maclaurin', 'Alexey Radul', 'Matthew J. Johnson', 'and Dimitrios Vytiniotis']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJlPdcY38B,Sparsity Programming: Automated Sparsity-Aware Optimizations in Differentiable Programming,"['program transformation', 'differentiable programming', 'automatic differentiation', 'sparsity detection']","Previous studies in numerical analysis have shown how the calculation of a Jacobian, Hessian, and their factorizations can be accelerated when their sparsity pattern is known. However, accurate Jacobian and Hessian sparsity patterns cannot be computed numerically, leaving the burden on the user to provide them.  As scientific simulations have grown in complexity, the application of differentiable programming for calculating the derivatives of arbitrary programs with respect to parameters has been on the rise, but current methodologies do not automatically detect and make use of sparsity-related accelerations. In this manuscript we develop a method for the accurate and efficient construction of sparsity patterns by transforming an input program into one that computes the sparsity pattern of its Jacobian or Hessian. Our implementation, which we demonstrate on partial differential equations, is a scalable technique for acceleration of automatic differentiation on arbitrarily complex multivariate programs. This demonstrates that dynamic program analysis can be effective in more scenarios than are currently well known in differentiable programming.",https://openreview.net/pdf/7f6572789097bbb84b989b4edb74d21731ee9007.pdf,stupidity,"['Shashi Gowda', 'Yingbo Ma', 'Valentin Churavy', 'Alan Edelman', 'Christopher Rackauckas']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=rJg_5AF6LB,Generalized Abs-Linear Learning,"['Abs-linear/normal form', 'piecewise linearization', 'dynamic search trajectory', 'back propagation']","We consider predictor functions $f(w;x)$ in abs-linear form, a generalization of neural nets with hinge activation. To train them with respect to a given data set of feature-label pairs $(x,y)$ one has to minimize the average loss, which is a multi-piecewise linear or quadratic function of the weights, i.e. coefficients of the abs-linear form.  We suggest to attack this nonsmooth global optimization problem via successive piecewise linearization, which allows the application of coordinate search, gradient based methods or mixed binary linear optimization. These alternative methods solve the sequence of abs-linear model problems with a proximal term as demonstrated in \cite{GrRoLOD}.  More general predictor functions $f(w;x)$ that are given in abs-normal form can be successively abs-linearized with respect to the weights $w$, which can then be optimized in a nested iteration. In the talk we will present numerical validations and comparisons with standard methods like ADAM \cite{adam}, e.g. on the MNIST problem.    ",https://openreview.net/pdf/3aae703bf6e8986a8c0cb553d0f2fe3b7e333be2.pdf,stupidity,"['Andreas Griewank', 'Ángel Rojas']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=r1xwBU5Vwr,Applications of a disintegration transformation,"['disintegration', 'probabilistic programming', 'density', 'MCMC']","We describe examples of applying a disintegration transformation on probabilistic programs to obtain posterior distributions, calculate symbolic representations of densities, and generate Markov Chain Monte Carlo (MCMC) samplers.",https://openreview.net/pdf/666f6a6955c916dd06dc5833b49d33c809f75116.pdf,stupidity,"['Praveen Narayanan', 'Chung-chieh Shan']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SygbjU6iBS,Transforming Probabilistic Programs into Algebraic Circuits for Inference and Learning,"['arithmetic circuits', 'knowledge compilation', 'probabilistic programming', 'probabilistic inference']","Probabilistic (logic) programs are routinely compiled into arithmetic circuits. During such a compilation step, the logic representation of a probabilistic program is transformed into an arithmetic representation. We show that this transformation and the resulting circuits cannot only be used for discrete probabilistic inference, but also for a number of other tasks such as differentiation, learning and probabilistic inference in the discrete-continuous domain.",https://openreview.net/pdf/f99c06e3f5ffa7a3c5b6bfbff10f634c9ec0e675.pdf,stupidity,"['Pedro Zuidberg Dos Martires', 'Vincent Derkinderen', 'Robin Manhaeve', 'Wannes Meert', 'Angelika Kimmig', 'Luc De Raedt']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SkxEF3FNPH,Taylor-Mode Automatic Differentiation for Higher-Order Derivatives in JAX,"['higher-order differentiation', 'taylor derivatives', 'automatic differentiation', 'neural ordinary differential equations']","One way to achieve higher-order automatic differentiation (AD) is to implement first-order AD and apply it repeatedly. This nested approach works, but can result in combinatorial amounts of redundant work. This paper describes a more efficient method, already known but with a new presentation, and its implementation in JAX. We also study its application to neural ordinary differential equations, and in particular discuss some additional algorithmic improvements for higher-order AD of differential equations.",https://openreview.net/pdf/025f91ed7c1059e6cb35b1704fc887452a4084f8.pdf,stupidity,"['Jesse Bettencourt', 'Matthew J. Johnson', 'David Duvenaud']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=SkluMSZ08H,Kotlin∇: A shape-safe DSL for differentiable programming,"['kotlin', 'differentiation', 'type safe']","Kotlin is a statically-typed programming language with support for embedded domain specific languages, asynchronous programming, and multi-platform compilation. In this work, we present an algebraically-based implementation of automatic differentiation (AD) with shape-safe tensor operations, written in pure Kotlin. Our approach differs from existing AD frameworks in that Kotlin∇ is the first shape-safe AD library fully compatible with the Java type system, requiring no metaprogramming, reflection or compiler intervention to use. A working prototype is available: https://github.com/breandan/kotlingrad.",https://openreview.net/pdf/5ca3ac3989b7e7d2a746d5d8e60071bfa9f84a78.pdf,stupidity,"['Breandan Considine', 'Michalis Famelis', 'Liam Paull']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1x4FtdVvS,TensorFlow as a Feature Engineering DSL,"['feature engineering', 'dsl', 'tensorflow', 'keras', 'target-rate encoding', 'one-hot encoding']","Deep Neural Networks have fulfilled the promise to reduce the need of
feature engineering by leveraging deep networks and training data. An
interesting side effect, maybe unexpected, is that frameworks for
implementing Deep Learning networks can also be used as frameworks for
feature engineering. We sketch here some ideas for a DSL implemented
on top of TensorFlow, hoping to attract interest from both the deep
learning computational graph and programming languages communities.",https://openreview.net/pdf/a0430e4ceaf02d501022f86c1e2bec7812ffd74d.pdf,stupidity,['Pablo Duboue'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=S1eeyLmaIH,WORD SEQUENCE PREDICTION FOR AMHARIC LANGUAGE,[],"Word prediction is one of the techniques to text entry and assistive technology for people with disabilities like Dyslexia which is problem of reading and spelling. For developing countries such as Ethiopia this kind of problems are neglected and the language spoken within the country are under resourced. Therefore applying AI to these problems has a major contribution. Amharic is used by a large number of populations, however no significant work is done on the topic of word sequence prediction. In this study, Amharic word sequence prediction model is developed with statistical methods using Hidden Markov Model by incorporating detailed parts of speech tag, some morphological features and user profiling or adaptation. Evaluation of the model is performed using developed prototype and keystroke savings (KSS) as a metrics. According to our experiment, prediction results using a bi-gram with morphological features and detailed Parts of Speech tag model has higher KSS and performed better compared those without specified features. Therefore, statistical approach with detailed POS, morphological features like gender, number, and person with suggested root or stem words using voice, tense, aspect, affixes statistical information and grammatical agreement rules of the language has quite good potential on word sequence Prediction for Amharic language.",https://openreview.net/pdf/c85eba94eaeef8858dec4e7128527cd5fab0a673.pdf,stupidity,"['Nuniyat Kifle', 'Ermias Abebe']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=HkecHuIaUS,Functional Tensors for Probabilistic Programming,"['probabilistic programming', 'symbolic algebra', 'approximate inference']","It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction, functional tensors, that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro probabilistic programming language. As an example application, we perform approximate inference on a switching linear dynamical system.",https://openreview.net/pdf/729c9791c2898b761f5d22f68bde377aa5b2ac10.pdf,stupidity,"['Fritz Obermeyer', 'Eli Bingham', 'Martin Jankowiak', 'Du Phan', 'Jonathan Chen']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1xiDw_pIr,Ad-Hoc Bayesian Program Learning,"['bayesian program learning', 'program induction', 'probabilistic program semantics']","Bayesian program learning provides a general approach to human-level concept learning in artificial intelligence.  However, most priors over powerful programming languages make searching for a high-scoring program intractable, and therefore cognitively unrealistic.  We hypothesize that an efficient learner searches programs which efficiently generate a likelihood by running to completion, and model this hypothesis with an ad-hoc proposal for programs.  Our proposal works backwards from observations to find programs which quickly generate similar results.",https://openreview.net/pdf/0d4499ab54a09cef12765f18d3c785013005da46.pdf,stupidity,['Eli Sennesh'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1xM0lu6LS,Approximations in Probabilistic Programs,[],"We introduce a new language construct, ""stat"", which converts the description of the Markov kernel of an ergodic Markov chain into a sample from its unique stationary distribution.  Up to minor changes in how certain error conditions are handled, we show that language constructs for soft-conditioning and normalization can be compiled away from the extended language. We then explore the problem of approximately implementing the semantics of the language with potentially nested ""stat"" expressions, in a language without ""stat"".
For a single ""stat"" term, the natural unrolling yields provable guarantees in an asymptotic sense. 
In the general case, under uniform ergodicity assumptions, we are able to give quantitative error bounds and convergence results for the approximate implementation of the extended first-order language. We leave open the question of whether the same guarantees can be made assuming mere geometric ergodicity.",https://openreview.net/pdf/62f5bd416970de4ddcb89fcc095d501565198f3b.pdf,stupidity,"['Ekansh Sharma', 'Daniel M. Roy']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=H1g1niFhIB,Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro,"['probabilistic programming', 'Markov Chain Monte Carlo', 'effect handlers', 'program transformations']","NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes.",https://openreview.net/pdf/f1ad873ecaf7861c3af6ecb7714c89ea365afe37.pdf,stupidity,"['Du Phan', 'Neeraj Pradhan', 'Martin Jankowiak']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=Bygj2Ys6IS,Automatic Differentiation: Inverse Accumulation Mode,['Newton step'],"We show that, under certain circumstances, it is possible to
automatically compute Jacobian-inverse-vector and
Jacobian-inverse-transpose-vector products about as efficiently as
Jacobian-vector and Jacobian-transpose-vector products.
The key insight is to notice that the Jacobian corresponding to the
use of one primitive arithmetic operator is of a form whose sparsity
is invariant to inversion.
This technique has the potential to allow the efficient direct
calculation of Newton steps.
",https://openreview.net/pdf/78413855d41d72155029422186083c2c49e8f181.pdf,stupidity,['Jeffrey Mark Siskind'],NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1lbS82pLS,Transforming recursive programs for parallel execution,"['batching', 'vectorization', 'compiler', 'stack', 'recursion', 'data-dependent', 'control flow']","We present a general approach to batching arbitrary computations for accelerators such as GPUs. We show orders-of-magnitude speedups using our method on the NoU-Turn Sampler (NUTS), a workhorse algorithm in Bayesian statistics. The central challenge of batching NUTS and other Markov chain Monte Carlo algorithms is data-dependent control flow and recursion.  We overcome this by mechanically transforming a single-example implementation into a form that explicitly tracks the current program point for each batch member, and only steps forward those in  the  same  place.   We  present  two  different  batching  algorithms:  a  simpler, previously published one that inherits recursion from the host Python, and a more complex, novel one that implements recursion directly and can batch across it.We implement these batching methods as a general program transformation onPython source. Both the batching system and the NUTS implementation presented here are available as part of the popular TensorFlow Probability software package.",https://openreview.net/pdf/fa582daf22348b482e26b7ee546a22e3fb71047d.pdf,stupidity,"['Alexey Radul', 'Brian Patton', 'Dougal Maclaurin', 'Matthew D. Hoffman', 'Rif A. Saurous']",NeurIPS.cc,2019,Workshop
https://openreview.net/forum?id=B1glaOr0US,Towards Polyhedral Automatic Differentiation,"['Automatic Differentiation', 'Polyhedral Compiler', 'Parallelisation', 'Vectorisation']","Most Automatic Differentiation (AD) tools lack a way to explicitly represent or differentiate performance-critical and hardware-dependent constructs such as parallelism, vectorisation, or tiling. Machine-learning frameworks work around this by hiding implementation details from the AD process, but lack the generality of general-purpose programming languages. Instead, this talk discusses the polyhedral model as a way for general-purpose AD tools to preserve  performance tweaks through the differentiation process.",https://openreview.net/pdf/4bfb424289a6b3cf6d1e418c26719627b59dbfba.pdf,stupidity,"['Jan Hückelheim', 'Navjot Kukreja']",NeurIPS.cc,2019,Workshop
